{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17RXKsBdmGu3"
   },
   "source": [
    "# 6 - Attention is All You Need\n",
    "\n",
    "In this notebook we will be implementing a (slightly modified version) of the Transformer model from the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. All images in this notebook will be taken from the Transformer paper. For more information about the Transformer, [see](https://www.mihaileric.com/posts/transformers-attention-in-disguise/) [these](https://jalammar.github.io/illustrated-transformer/) [three](http://nlp.seas.harvard.edu/2018/04/03/attention.html) articles.\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/transformer1.png?raw=1)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Similar to the Convolutional Sequence-to-Sequence model, the Transformer does not use any recurrence. It also does not use any convolutional layers. Instead the model is entirely made up of linear layers, attention mechanisms and normalization. \n",
    "\n",
    "As of January 2020, Transformers are the dominant architecture in NLP and are used to achieve state-of-the-art results for many tasks and it appears as if they will be for the near future. \n",
    "\n",
    "The most popular Transformer variant is [BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) and pre-trained versions of BERT are commonly used to replace the embedding layers - if not more - in NLP models. \n",
    "\n",
    "A common library used when dealing with pre-trained transformers is the [Transformers](https://huggingface.co/transformers/) library, see [here](https://huggingface.co/transformers/pretrained_models.html) for a list of all pre-trained models available.\n",
    "\n",
    "The differences between the implementation in this notebook and the paper are:\n",
    "- we use a learned positional encoding instead of a static one\n",
    "- we use the standard Adam optimizer with a static learning rate instead of one with warm-up and cool-down steps\n",
    "- we do not use label smoothing\n",
    "\n",
    "We make all of these changes as they closely follow BERT's set-up and the majority of Transformer variants use a similar set-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRg6SbqJmGu9"
   },
   "source": [
    "## Preparing the Data\n",
    "\n",
    "As always, let's import all the required modules and set the random seeds for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "id": "Xd_Bx1UZmGu9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "id": "quJyJMZ7mGu_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtW_21NPmGu_"
   },
   "source": [
    "We'll then create our tokenizers as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "id": "vuR10rqwmGvA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "id": "oeON6EiamGvB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71RTPHLGmGvC"
   },
   "source": [
    "Our fields are the same as the previous notebook. The model expects data to be fed in with the batch dimension first, so we use `batch_first = True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "id": "W9PGWYY9mGvC",
    "outputId": "e2e35a27-eb8a-4a43-cb71-bd26c8bacb56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KznyZIR0mGvE"
   },
   "source": [
    "We then load the Multi30k dataset and build the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "id": "Oqlxt7mDmGvE",
    "outputId": "f8650b97-4c52-4827-efab-382dea773438",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.legacy.datasets.translation.Multi30k'>\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))\n",
    "\n",
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "id": "JllDiGt6mGvF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLwJQ1psmGvF"
   },
   "source": [
    "Finally, we define the device and the data iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "id": "Dg1mj69UmGvF",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "id": "ZCtTmETEmGvG",
    "outputId": "cbf7f605-edfb-4d16-b295-bc3e57410917",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 4196\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4DkLpMlmGvG"
   },
   "source": [
    "## Building the Model\n",
    "\n",
    "Next, we'll build the model. Like previous notebooks it is made up of an *encoder* and a *decoder*, with the encoder *encoding* the input/source sentence (in German) into *context vector* and the decoder then *decoding* this context vector to output our output/target sentence (in English). \n",
    "\n",
    "### Encoder\n",
    "\n",
    "Similar to the ConvSeq2Seq model, the Transformer's encoder does not attempt to compress the entire source sentence, $X = (x_1, ... ,x_n)$, into a single context vector, $z$. Instead it produces a sequence of context vectors, $Z = (z_1, ... , z_n)$. So, if our input sequence was 5 tokens long we would have $Z = (z_1, z_2, z_3, z_4, z_5)$. Why do we call this a sequence of context vectors and not a sequence of hidden states? A hidden state at time $t$ in an RNN has only seen tokens $x_t$ and all the tokens before it. However, each context vector here has seen all tokens at all positions within the input sequence.\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/transformer-encoder.png?raw=1)\n",
    "\n",
    "First, the tokens are passed through a standard embedding layer. Next, as the model has no recurrent it has no idea about the order of the tokens within the sequence. We solve this by using a second embedding layer called a *positional embedding layer*. This is a standard embedding layer where the input is not the token itself but the position of the token within the sequence, starting with the first token, the `<sos>` (start of sequence) token, in position 0. The position embedding has a \"vocabulary\" size of 100, which means our model can accept sentences up to 100 tokens long. This can be increased if we want to handle longer sentences.\n",
    "\n",
    "The original Transformer implementation from the Attention is All You Need paper does not learn positional embeddings. Instead it uses a fixed static embedding. Modern Transformer architectures, like BERT, use positional embeddings instead, hence we have decided to use them in these tutorials. Check out [this](http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding) section to read more about the positional embeddings used in the original Transformer model.\n",
    "\n",
    "Next, the token and positional embeddings are elementwise summed together to get a vector which contains information about the token and also its position with in the sequence. However, before they are summed, the token embeddings are multiplied by a scaling factor which is $\\sqrt{d_{model}}$, where $d_{model}$ is the hidden dimension size, `hid_dim`. This supposedly reduces variance in the embeddings and the model is difficult to train reliably without this scaling factor. Dropout is then applied to the combined embeddings.\n",
    "\n",
    "The combined embeddings are then passed through $N$ *encoder layers* to get $Z$, which is then output and can be used by the decoder.\n",
    "\n",
    "The source mask, `src_mask`, is simply the same shape as the source sentence but has a value of 1 when the token in the source sentence is not a `<pad>` token and 0 when it is a `<pad>` token. This is used in the encoder layers to mask the multi-head attention mechanisms, which are used to calculate and apply attention over the source sentence, so the model does not pay attention to `<pad>` tokens, which contain no useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "id": "OHt0vVddmGvH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim,\n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        #src = [batch size, src len, hid dim]\n",
    "            \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kk38v_oKmGvH"
   },
   "source": [
    "### Encoder Layer\n",
    "\n",
    "The encoder layers are where all of the \"meat\" of the encoder is contained. We first pass the source sentence and its mask into the *multi-head attention layer*, then perform dropout on it, apply a residual connection and pass it through a [Layer Normalization](https://arxiv.org/abs/1607.06450) layer. We then pass it through a *position-wise feedforward* layer and then, again, apply dropout, a residual connection and then layer normalization to get the output of this layer which is fed into the next layer. The parameters are not shared between layers. \n",
    "\n",
    "The mutli head attention layer is used by the encoder layer to attend to the source sentence, i.e. it is calculating and applying attention over itself instead of another sequence, hence we call it *self attention*.\n",
    "\n",
    "[This](https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/) article goes into more detail about layer normalization, but the gist is that it normalizes the values of the features, i.e. across the hidden dimension, so each feature has a mean of 0 and a standard deviation of 1. This allows neural networks with a larger number of layers, like the Transformer, to be trained easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "id": "wCtqoOGOmGvI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len] \n",
    "                \n",
    "        #self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centered Kernel Alignment (CKA)\n",
    "\n",
    "To quantify the inter-head diversity an indicator called Centered Kernel Alignment (CKA) is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def linear_kernel_torch(X):\n",
    "    #size_X = X.size()\n",
    "    #dim_ind = list(range(len(size_X)))\n",
    "    # permute dim_ind\n",
    "    #dim_ind[-2] = dim_ind[-1] \n",
    "    #dim_ind[-1] = len(size_X) -2 \n",
    "    return torch.matmul(X, X.t())\n",
    "\n",
    "def centering_torch(X):\n",
    "    if torch.__version__ < '1.2':  ## python 2, torch 1.1\n",
    "        means = torch.mean(X, [1])\n",
    "    else:\n",
    "        means = torch.mean(X, axis=0)\n",
    "        means -= torch.mean(means) / 2.\n",
    "\n",
    "    centered_X = X - means[:, None]\n",
    "    centered_X -= means[None, :]\n",
    "\n",
    "    return centered_X\n",
    "\n",
    "def hsic_torch(X, Y, kernel=linear_kernel_torch):\n",
    "    gram_X = kernel(X)\n",
    "    gram_Y = kernel(Y)\n",
    "\n",
    "    centered_gram_X = centering_torch(gram_X)\n",
    "    centered_gram_Y = centering_torch(gram_Y)\n",
    "\n",
    "    scaled_hsic = torch.dot(torch.flatten(centered_gram_X), torch.flatten(centered_gram_Y))\n",
    "\n",
    "    return scaled_hsic\n",
    "\n",
    "def cka_torch(X, Y, kernel=linear_kernel_torch):\n",
    "#     d_type = X.type()\n",
    "#     assert X.type() == Y.type()\n",
    "\n",
    "#     X = X.type(torch.float64)\n",
    "#     Y = Y.type(torch.float64)\n",
    "\n",
    "    gram_X = kernel(X)\n",
    "    gram_Y = kernel(Y)\n",
    "\n",
    "    centered_gram_X = centering_torch(gram_X)\n",
    "    centered_gram_Y = centering_torch(gram_Y)\n",
    "\n",
    "    scaled_hsic = torch.dot(torch.flatten(centered_gram_X), torch.flatten(centered_gram_Y))    \n",
    "\n",
    "    norm_X = torch.linalg.norm(centered_gram_X)\n",
    "    norm_Y = torch.linalg.norm(centered_gram_Y)\n",
    "\n",
    "    cka_value = scaled_hsic / (norm_X * norm_Y)\n",
    "    cka_value = cka_value.type(d_type)\n",
    "    return cka_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#NUMPY\n",
    "def linear_kernel(X):\n",
    "    return np.dot(X, X.transpose())\n",
    "\n",
    "\n",
    "def centering(X):\n",
    "    if not np.allclose(X, X.T):\n",
    "        raise ValueError('Input must be a symmetric matrix.')\n",
    "\n",
    "    means = np.mean(X, 0)\n",
    "    means -= np.mean(means) / 2\n",
    "\n",
    "    centered_X = X - means[:, None]\n",
    "    centered_X -= means[None, :]\n",
    "\n",
    "    return centered_X\n",
    "\n",
    "\n",
    "def hsic(X, Y, kernel=linear_kernel):\n",
    "    gram_X = kernel(X)\n",
    "    gram_Y = kernel(Y)\n",
    "\n",
    "    centered_gram_X = centering(gram_X)\n",
    "    centered_gram_Y = centering(gram_Y)\n",
    "\n",
    "    scaled_hsic = np.dot(np.ravel(centered_gram_X), np.ravel(centered_gram_Y))\n",
    "\n",
    "    return scaled_hsic\n",
    "\n",
    "\n",
    "def cka(X, Y, kernel=linear_kernel):\n",
    "    gram_X = kernel(X)\n",
    "    gram_Y = kernel(Y)\n",
    "\n",
    "    centered_gram_X = centering(gram_X)\n",
    "    centered_gram_Y = centering(gram_Y)\n",
    "\n",
    "    scaled_hsic = np.dot(np.ravel(centered_gram_X), np.ravel(centered_gram_Y))\n",
    "\n",
    "    norm_X = np.linalg.norm(centered_gram_X)\n",
    "    norm_Y = np.linalg.norm(centered_gram_Y)\n",
    "\n",
    "    return scaled_hsic / (norm_X * norm_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIAnpQ56mGvI"
   },
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "One of the key, novel concepts introduced by the Transformer paper is the *multi-head attention layer*. \n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/transformer-attention.png?raw=1)\n",
    "\n",
    "Attention can be though of as *queries*, *keys* and *values* - where the query is used with the key to get an attention vector (usually the output of a *softmax* operation and has all values between 0 and 1 which sum to 1) which is then used to get a weighted sum of the values.\n",
    "\n",
    "The Transformer uses *scaled dot-product attention*, where the query and key are combined by taking the dot product between them, then applying the softmax operation and scaling by $d_k$ before finally then multiplying by the value. $d_k$ is the *head dimension*, `head_dim`, which we will shortly explain further.\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ \n",
    "\n",
    "This is similar to standard *dot product attention* but is scaled by $d_k$, which the paper states is used to stop the results of the dot products growing large, causing gradients to become too small.\n",
    "\n",
    "However, the scaled dot-product attention isn't simply applied to the queries, keys and values. Instead of doing a single attention application the queries, keys and values have their `hid_dim` split into $h$ *heads* and the scaled dot-product attention is calculated over all heads in parallel. This means instead of paying attention to one concept per attention application, we pay attention to $h$. We then re-combine the heads into their `hid_dim` shape, thus each `hid_dim` is potentially paying attention to $h$ different concepts.\n",
    "\n",
    "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O $$\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n",
    "\n",
    "$W^O$ is the linear layer applied at the end of the multi-head attention layer, `fc`. $W^Q, W^K, W^V$ are the linear layers `fc_q`, `fc_k` and `fc_v`.\n",
    "\n",
    "Walking through the module, first we calculate $QW^Q$, $KW^K$ and $VW^V$ with the linear layers, `fc_q`, `fc_k` and `fc_v`, to give us `Q`, `K` and `V`. Next, we split the `hid_dim` of the query, key and value into `n_heads` using `.view` and correctly permute them so they can be multiplied together. We then calculate the `energy` (the un-normalized attention) by multiplying `Q` and `K` together and scaling it by the square root of `head_dim`, which is calulated as `hid_dim // n_heads`. We then mask the energy so we do not pay attention over any elements of the sequeuence we shouldn't, then apply the softmax and dropout. We then apply the attention to the value heads, `V`, before combining the `n_heads` together. Finally, we multiply this $W^O$, represented by `fc_o`. \n",
    "\n",
    "Note that in our implementation the lengths of the keys and values are always the same, thus when matrix multiplying the output of the softmax, `attention`, with `V` we will always have valid dimension sizes for matrix multiplication. This multiplication is carried out using `torch.matmul` which, when both tensors are >2-dimensional, does a batched matrix multiplication over the last two dimensions of each tensor. This will be a **[query len, key len] x [value len, head dim]** batched matrix multiplication over the batch size and each head which provides the **[batch size, n heads, query len, head dim]** result.\n",
    "\n",
    "One thing that looks strange at first is that dropout is applied directly to the attention. This means that our attention vector will most probably not sum to 1 and we may pay full attention to a token but the attention over that token is set to 0 by dropout. This is never explained, or even mentioned, in the paper however is used by the [official implementation](https://github.com/tensorflow/tensor2tensor/) and every Transformer implementation since, [including BERT](https://github.com/google-research/bert/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "id": "uyFU5LfOmGvJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.counter = 0\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "                \n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #### THIS IS THE ATTENTION MATRIX ####\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        self.counter += 1\n",
    "        print(f\"counter ##################{self.counter}######################\")\n",
    "        #print(f\"[batch size, n heads, query len, head dim]\")\n",
    "        #print(f\"x after matmul{x.shape}\")\n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        #print(f\"[batch size, query len, n heads, head dim]\")\n",
    "        #print(f\"x after permute{x.shape}\")      \n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        #add CKA computation here\n",
    "        head_matrices = np.split(x, self.n_heads , axis = 2)   \n",
    "        cka_heads = []\n",
    "        for i in range(len(head_matrices)):\n",
    "            for j in range(len(head_matrices)):\n",
    "                if i != j:\n",
    "                    array_x = np.array(head_matrices[i].detach().numpy())\n",
    "                    array_x = np.squeeze(array_x, axis = 2)\n",
    "                    array_y = np.array(head_matrices[j].detach().numpy())\n",
    "                    array_y = np.squeeze(array_y, axis = 2)\n",
    "                    array_x = array_x.reshape((array_x.shape[0]*array_x.shape[1],array_x.shape[2]))\n",
    "                    array_y = array_y.reshape((array_y.shape[0]*array_y.shape[1],array_y.shape[2]))\n",
    "                    cka_heads.append(cka(array_x, array_y, kernel=linear_kernel))\n",
    "        print(cka_heads)\n",
    "        \n",
    "        \n",
    "        ###heads are combined here:\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        x = self.fc_o(x)\n",
    "        #print(f\"[batch size, query len, hid dim]\")\n",
    "        #print(f\"x final{x.shape}\")\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#mport numpy as np\n",
    "# The matrix we have:\n",
    "## [batch size, query len, n heads, head dim]\n",
    "## [128, 23, 2, 128]\n",
    "#arr4d_org = np.arange(753664).reshape(128, 23, 2, 128)\n",
    "\n",
    "\n",
    "# A test matrix of the same shape to help figure out the right split:\n",
    "## [batch size, query len, n heads, head dim]\n",
    "## [2, 3, 4, 5]\n",
    "#arr4d_test = np.arange(120).reshape(2, 3, 4, 5)\n",
    "\n",
    "\"\"\"\n",
    "[[[[  0   1   2   3   4]\n",
    "   [  5   6   7   8   9]\n",
    "   [ 10  11  12  13  14]\n",
    "   [ 15  16  17  18  19]]\n",
    "\n",
    "  [[ 20  21  22  23  24]\n",
    "   [ 25  26  27  28  29]\n",
    "   [ 30  31  32  33  34]\n",
    "   [ 35  36  37  38  39]]\n",
    "\n",
    "  [[ 40  41  42  43  44]\n",
    "   [ 45  46  47  48  49]\n",
    "   [ 50  51  52  53  54]\n",
    "   [ 55  56  57  58  59]]]\n",
    "\n",
    "\n",
    " [[[ 60  61  62  63  64]\n",
    "   [ 65  66  67  68  69]\n",
    "   [ 70  71  72  73  74]\n",
    "   [ 75  76  77  78  79]]\n",
    "\n",
    "  [[ 80  81  82  83  84]\n",
    "   [ 85  86  87  88  89]\n",
    "   [ 90  91  92  93  94]\n",
    "   [ 95  96  97  98  99]]\n",
    "\n",
    "  [[100 101 102 103 104]\n",
    "   [105 106 107 108 109]\n",
    "   [110 111 112 113 114]\n",
    "   [115 116 117 118 119]]]]\n",
    "   \n",
    "\"\"\"\n",
    "# The test matrix split to get what we want: a matrix per head\n",
    "head_matrices = np.split(arr4d_test, arr4d_test.shape[2], axis = 2)\n",
    "#head_matrices = np.split(arr4d_org, arr4d_org.shape[2], axis = 2)\n",
    "\n",
    "\n",
    "\n",
    "#cka_torch(X, Y, kernel=linear_kernel_torch)\n",
    "#print(np.dtype(head_matrices))\n",
    "\n",
    "\n",
    "# array_x = np.array(head_matrices[0])\n",
    "# array_y = np.array(head_matrices[1])\n",
    "# array_x = np.squeeze(array_x, axis = 2)\n",
    "# array_y = np.squeeze(array_y, axis = 2)\n",
    "# array_x = array_x.reshape((array_x.shape[0]*array_x.shape[1],array_x.shape[2]))\n",
    "# array_y = array_y.reshape((array_y.shape[0]*array_y.shape[1],array_y.shape[2]))\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(len(head_matrices)):\n",
    "#     for j in range(len(head_matrices)):\n",
    "#         if i != j:\n",
    "#             array_x = np.array(head_matrices[i])\n",
    "#             array_x = np.squeeze(array_x, axis = 2)\n",
    "#             array_y = np.array(head_matrices[j])\n",
    "#             array_y = np.squeeze(array_y, axis = 2)\n",
    "#             array_x = array_x.reshape((array_x.shape[0]*array_x.shape[1],array_x.shape[2]))\n",
    "#             array_y = array_y.reshape((array_y.shape[0]*array_y.shape[1],array_y.shape[2]))\n",
    "#             print(cka(array_x, array_y, kernel=linear_kernel))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9MBbfqomGvJ"
   },
   "source": [
    "### Position-wise Feedforward Layer\n",
    "\n",
    "The other main block inside the encoder layer is the *position-wise feedforward layer* This is relatively simple compared to the multi-head attention layer. The input is transformed from `hid_dim` to `pf_dim`, where `pf_dim` is usually a lot larger than `hid_dim`. The original Transformer used a `hid_dim` of 512 and a `pf_dim` of 2048. The ReLU activation function and dropout are applied before it is transformed back into a `hid_dim` representation. \n",
    "\n",
    "Why is this used? Unfortunately, it is never explained in the paper.\n",
    "\n",
    "BERT uses the [GELU](https://arxiv.org/abs/1606.08415) activation function, which can be used by simply switching `torch.relu` for `F.gelu`. Why did they use GELU? Again, it is never explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "id": "RiY7iH08mGvK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akXKEiZwmGvK"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The objective of the decoder is to take the encoded representation of the source sentence, $Z$, and convert it into predicted tokens in the target sentence, $\\hat{Y}$. We then compare $\\hat{Y}$ with the actual tokens in the target sentence, $Y$, to calculate our loss, which will be used to calculate the gradients of our parameters and then use our optimizer to update our weights in order to improve our predictions. \n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/transformer-decoder.png?raw=1)\n",
    "\n",
    "The decoder is similar to encoder, however it now has two multi-head attention layers. A *masked multi-head attention layer* over the target sequence, and a multi-head attention layer which uses the decoder representation as the query and the encoder representation as the key and value.\n",
    "\n",
    "The decoder uses positional embeddings and combines - via an elementwise sum - them with the scaled embedded target tokens, followed by dropout. Again, our positional encodings have a \"vocabulary\" of 100, which means they can accept sequences up to 100 tokens long. This can be increased if desired.\n",
    "\n",
    "The combined embeddings are then passed through the $N$ decoder layers, along with the encoded source, `enc_src`, and the source and target masks. Note that the number of layers in the encoder does not have to be equal to the number of layers in the decoder, even though they are both denoted by $N$.\n",
    "\n",
    "The decoder representation after the $N^{th}$ layer is then passed through a linear layer, `fc_out`. In PyTorch, the softmax operation is contained within our loss function, so we do not explicitly need to use a softmax layer here.\n",
    "\n",
    "As well as using the source mask, as we did in the encoder to prevent our model attending to `<pad>` tokens, we also use a target mask. This will be explained further in the `Seq2Seq` model which encapsulates both the encoder and decoder, but the gist of it is that it performs a similar operation as the decoder padding in the convolutional sequence-to-sequence model. As we are processing all of the target tokens at once in parallel we need a method of stopping the decoder from \"cheating\" by simply \"looking\" at what the next token in the target sequence is and outputting it. \n",
    "\n",
    "Our decoder layer also outputs the normalized attention values so we can later plot them to see what our model is actually paying attention to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "id": "vIWfdT5omGvK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "                            \n",
    "        #pos = [batch size, trg len]\n",
    "            \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "                \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaf5IAt1mGvK"
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "As mentioned previously, the decoder layer is similar to the encoder layer except that it now has two multi-head attention layers, `self_attention` and `encoder_attention`. \n",
    "\n",
    "The first performs self-attention, as in the encoder, by using the decoder representation so far as the query, key and value. This is followed by dropout, residual connection and layer normalization. This `self_attention` layer uses the target sequence mask, `trg_mask`, in order to prevent the decoder from \"cheating\" by paying attention to tokens that are \"ahead\" of the one it is currently processing as it processes all tokens in the target sentence in parallel.\n",
    "\n",
    "The second is how we actually feed the encoded source sentence, `enc_src`, into our decoder. In this multi-head attention layer the queries are the decoder representations and the keys and values are the encoder representations. Here, the source mask, `src_mask` is used to prevent the multi-head attention layer from attending to `<pad>` tokens within the source sentence. This is then followed by the dropout, residual connection and layer normalization layers. \n",
    "\n",
    "Finally, we pass this through the position-wise feedforward layer and yet another sequence of dropout, residual connection and layer normalization.\n",
    "\n",
    "The decoder layer isn't introducing any new concepts, just using the same set of layers as the encoder in a slightly different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "id": "8V5rXVAymGvL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        #self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "            \n",
    "        #encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "                    \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGm3nenfmGvL"
   },
   "source": [
    "### Seq2Seq\n",
    "\n",
    "Finally, we have the `Seq2Seq` module which encapsulates the encoder and decoder, as well as handling the creation of the masks.\n",
    "\n",
    "The source mask is created by checking where the source sequence is not equal to a `<pad>` token. It is 1 where the token is not a `<pad>` token and 0 when it is. It is then unsqueezed so it can be correctly broadcast when applying the mask to the `energy`, which of shape **_[batch size, n heads, seq len, seq len]_**.\n",
    "\n",
    "The target mask is slightly more complicated. First, we create a mask for the `<pad>` tokens, as we did for the source mask. Next, we create a \"subsequent\" mask, `trg_sub_mask`, using `torch.tril`. This creates a diagonal matrix where the elements above the diagonal will be zero and the elements below the diagonal will be set to whatever the input tensor is. In this case, the input tensor will be a tensor filled with ones. So this means our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "This shows what each target token (row) is allowed to look at (column). The first target token has a mask of **_[1, 0, 0, 0, 0]_** which means it can only look at the first target token. The second target token has a mask of **_[1, 1, 0, 0, 0]_** which it means it can look at both the first and second target tokens. \n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "After the masks are created, they used with the encoder and decoder along with the source and target sentences to get our predicted target sentence, `output`, along with the decoder's attention over the source sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "id": "FHdHKYMzmGvL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        \n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        \n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlJ_IxmwmGvM"
   },
   "source": [
    "## Training the Seq2Seq Model\n",
    "\n",
    "We can now define our encoder and decoders. This model is significantly smaller than Transformers used in research today, but is able to be run on a single GPU quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "id": "ys6IfZ5ZmGvM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 2\n",
    "DEC_HEADS = 2\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1400rbwmGvM"
   },
   "source": [
    "Then, use them to define our whole sequence-to-sequence encapsulating model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "id": "FiN3gBdJmGvM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWbvJboGmGvM"
   },
   "source": [
    "We can check the number of parameters, noticing it is significantly less than the 37M for the convolutional sequence-to-sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "id": "1gPOi2OamGvM",
    "outputId": "980f81d6-d163-49fc-b163-04e6f6d95c4e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,038,341 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksUMTrvKmGvN"
   },
   "source": [
    "The paper does not mention which weight initialization scheme was used, however Xavier uniform seems to be common amongst Transformer models, so we use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "id": "x5eUmLKomGvN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "id": "Z5VjbR2zmGvN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn65_V8DmGvN"
   },
   "source": [
    "The optimizer used in the original Transformer paper uses Adam with a learning rate that has a \"warm-up\" and then a \"cool-down\" period. BERT and other Transformer models use Adam with a fixed learning rate, so we will implement that. Check [this](http://nlp.seas.harvard.edu/2018/04/03/attention.html#optimizer) link for more details about the original Transformer's learning rate schedule.\n",
    "\n",
    "Note that the learning rate needs to be lower than the default used by Adam or else learning is unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "id": "_usdxEkomGvO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzZHEZXgmGvO"
   },
   "source": [
    "Next, we define our loss function, making sure to ignore losses calculated over `<pad>` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "id": "sPiANrGnmGvO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXnTpP7HmGvO"
   },
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "id": "pBWlOfJJmGvO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    speeder = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        speeder += 1\n",
    "        if speeder == 3:\n",
    "            return epoch_loss / len(iterator)\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0wTWCGDmGvP"
   },
   "source": [
    "The evaluation loop is the same as the training loop, just without the gradient calculations and parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "id": "8DdE3lQ7mGvP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        speeder = 0    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            speeder += 1\n",
    "            if speeder == 3:\n",
    "                return epoch_loss / len(iterator)\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diHbqqtImGvP"
   },
   "source": [
    "We then define a small function that we can use to tell us how long an epoch takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "id": "bNiZxzklmGvQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytLAjNejmGvQ"
   },
   "source": [
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "id": "75751kIGmGvQ",
    "outputId": "d5b35509-231d-4f01-87cc-35347ff642fc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter ##################1######################\n",
      "[0.60434574, 0.60434574]\n",
      "counter ##################1######################\n",
      "[0.34780943, 0.34780943]\n",
      "counter ##################1######################\n",
      "[0.51378113, 0.51378113]\n",
      "counter ##################1######################\n",
      "[0.7138121, 0.7138121]\n",
      "counter ##################1######################\n",
      "[0.6296978, 0.6296978]\n",
      "counter ##################1######################\n",
      "[0.4380886, 0.4380886]\n",
      "counter ##################1######################\n",
      "[0.64299655, 0.64299655]\n",
      "counter ##################1######################\n",
      "[0.5122316, 0.5122316]\n",
      "counter ##################1######################\n",
      "[0.6250429, 0.6250429]\n",
      "counter ##################2######################\n",
      "[0.5869158, 0.5869158]\n",
      "counter ##################2######################\n",
      "[0.38006622, 0.38006622]\n",
      "counter ##################2######################\n",
      "[0.5366315, 0.5366315]\n",
      "counter ##################2######################\n",
      "[0.6937565, 0.6937565]\n",
      "counter ##################2######################\n",
      "[0.64103496, 0.64103496]\n",
      "counter ##################2######################\n",
      "[0.3604633, 0.3604633]\n",
      "counter ##################2######################\n",
      "[0.6045246, 0.6045246]\n",
      "counter ##################2######################\n",
      "[0.28073725, 0.28073725]\n",
      "counter ##################2######################\n",
      "[0.62400967, 0.62400967]\n",
      "counter ##################3######################\n",
      "[0.7413186, 0.7413186]\n",
      "counter ##################3######################\n",
      "[0.49617174, 0.49617174]\n",
      "counter ##################3######################\n",
      "[0.77234966, 0.77234966]\n",
      "counter ##################3######################\n",
      "[0.94760966, 0.94760966]\n",
      "counter ##################3######################\n",
      "[0.81599355, 0.81599355]\n",
      "counter ##################3######################\n",
      "[0.65980566, 0.65980566]\n",
      "counter ##################3######################\n",
      "[0.8364748, 0.8364748]\n",
      "counter ##################3######################\n",
      "[0.8311363, 0.8311363]\n",
      "counter ##################3######################\n",
      "[0.83902293, 0.83902293]\n",
      "counter ##################4######################\n",
      "[0.7702624, 0.7702624]\n",
      "counter ##################4######################\n",
      "[0.5394888, 0.5394888]\n",
      "counter ##################4######################\n",
      "[0.8054093, 0.8054093]\n",
      "counter ##################4######################\n",
      "[0.9528148, 0.9528148]\n",
      "counter ##################4######################\n",
      "[0.8457498, 0.8457498]\n",
      "counter ##################4######################\n",
      "[0.63274825, 0.63274825]\n",
      "counter ##################4######################\n",
      "[0.8657408, 0.8657408]\n",
      "counter ##################4######################\n",
      "[0.8407512, 0.8407512]\n",
      "counter ##################4######################\n",
      "[0.83943844, 0.83943844]\n",
      "Epoch: 01 | Time: 0m 30s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 1.982 |  Val. PPL:   7.257\n",
      "counter ##################5######################\n",
      "[0.57701796, 0.57701796]\n",
      "counter ##################5######################\n",
      "[0.36366606, 0.36366606]\n",
      "counter ##################5######################\n",
      "[0.49487326, 0.49487326]\n",
      "counter ##################5######################\n",
      "[0.67688304, 0.67688304]\n",
      "counter ##################5######################\n",
      "[0.49944076, 0.49944076]\n",
      "counter ##################5######################\n",
      "[0.28762975, 0.28762975]\n",
      "counter ##################5######################\n",
      "[0.4635024, 0.4635024]\n",
      "counter ##################5######################\n",
      "[0.10185754, 0.10185754]\n",
      "counter ##################5######################\n",
      "[0.48980206, 0.48980206]\n",
      "counter ##################6######################\n",
      "[0.5821669, 0.5821669]\n",
      "counter ##################6######################\n",
      "[0.36916482, 0.36916482]\n",
      "counter ##################6######################\n",
      "[0.43702096, 0.43702096]\n",
      "counter ##################6######################\n",
      "[0.72755754, 0.72755754]\n",
      "counter ##################6######################\n",
      "[0.3730053, 0.3730053]\n",
      "counter ##################6######################\n",
      "[0.26237038, 0.26237038]\n",
      "counter ##################6######################\n",
      "[0.3156271, 0.3156271]\n",
      "counter ##################6######################\n",
      "[0.055105343, 0.055105343]\n",
      "counter ##################6######################\n",
      "[0.3509942, 0.3509942]\n",
      "counter ##################7######################\n",
      "[0.74399596, 0.74399596]\n",
      "counter ##################7######################\n",
      "[0.5347299, 0.5347299]\n",
      "counter ##################7######################\n",
      "[0.819774, 0.819774]\n",
      "counter ##################7######################\n",
      "[0.9269334, 0.9269334]\n",
      "counter ##################7######################\n",
      "[0.84804225, 0.84804225]\n",
      "counter ##################7######################\n",
      "[0.67500174, 0.67500174]\n",
      "counter ##################7######################\n",
      "[0.84278554, 0.84278554]\n",
      "counter ##################7######################\n",
      "[0.8656305, 0.8656305]\n",
      "counter ##################7######################\n",
      "[0.8615085, 0.8615085]\n",
      "counter ##################8######################\n",
      "[0.7711025, 0.7711025]\n",
      "counter ##################8######################\n",
      "[0.57630706, 0.57630706]\n",
      "counter ##################8######################\n",
      "[0.8396376, 0.8396376]\n",
      "counter ##################8######################\n",
      "[0.9235776, 0.9235776]\n",
      "counter ##################8######################\n",
      "[0.867007, 0.867007]\n",
      "counter ##################8######################\n",
      "[0.6366277, 0.6366277]\n",
      "counter ##################8######################\n",
      "[0.87361753, 0.87361753]\n",
      "counter ##################8######################\n",
      "[0.8697249, 0.8697249]\n",
      "counter ##################8######################\n",
      "[0.85347813, 0.85347813]\n",
      "Epoch: 02 | Time: 0m 45s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 1.904 |  Val. PPL:   6.709\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQOjJKCtmGvR"
   },
   "source": [
    "We load our \"best\" parameters and manage to achieve a better test perplexity than all previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "id": "Nfe2iuPJmGvR",
    "outputId": "46441cd5-8bb1-4089-e40f-6d890b800812",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter ##################9######################\n",
      "[0.76229584, 0.76229584]\n",
      "counter ##################9######################\n",
      "[0.5683929, 0.5683929]\n",
      "counter ##################9######################\n",
      "[0.8262935, 0.8262935]\n",
      "counter ##################9######################\n",
      "[0.92621547, 0.92621547]\n",
      "counter ##################9######################\n",
      "[0.8569533, 0.8569533]\n",
      "counter ##################9######################\n",
      "[0.69048053, 0.69048053]\n",
      "counter ##################9######################\n",
      "[0.86765534, 0.86765534]\n",
      "counter ##################9######################\n",
      "[0.87025326, 0.87025326]\n",
      "counter ##################9######################\n",
      "[0.86268044, 0.86268044]\n",
      "counter ##################10######################\n",
      "[0.7451503, 0.7451503]\n",
      "counter ##################10######################\n",
      "[0.5324194, 0.5324194]\n",
      "counter ##################10######################\n",
      "[0.8110301, 0.8110301]\n",
      "counter ##################10######################\n",
      "[0.92703074, 0.92703074]\n",
      "counter ##################10######################\n",
      "[0.8576525, 0.8576525]\n",
      "counter ##################10######################\n",
      "[0.6597292, 0.6597292]\n",
      "counter ##################10######################\n",
      "[0.84204483, 0.84204483]\n",
      "counter ##################10######################\n",
      "[0.87151897, 0.87151897]\n",
      "counter ##################10######################\n",
      "[0.8527344, 0.8527344]\n",
      "| Test Loss: 1.908 | Test PPL:   6.737 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut6-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EwZboelmGvR"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Now we can can translations from our model with the `translate_sentence` function below.\n",
    "\n",
    "The steps taken are:\n",
    "- tokenize the source sentence if it has not been tokenized (is a string)\n",
    "- append the `<sos>` and `<eos>` tokens\n",
    "- numericalize the source sentence\n",
    "- convert it to a tensor and add a batch dimension\n",
    "- create the source sentence mask\n",
    "- feed the source sentence and mask into the encoder\n",
    "- create a list to hold the output sentence, initialized with an `<sos>` token\n",
    "- while we have not hit a maximum length\n",
    "  - convert the current output sentence prediction into a tensor with a batch dimension\n",
    "  - create a target sentence mask\n",
    "  - place the current output, encoder output and both masks into the decoder\n",
    "  - get next output token prediction from decoder along with attention\n",
    "  - add prediction to current output sentence prediction\n",
    "  - break if the prediction was an `<eos>` token\n",
    "- convert the output sentence from indexes to tokens\n",
    "- return the output sentence (with the `<sos>` token removed) and the attention from the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "id": "hw8NK2bLmGvR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de_core_news_sm')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MkgR6pzmGvS"
   },
   "source": [
    "We'll now define a function that displays the attention over the source sentence for each step of the decoding. As this model has 8 heads our model we can view the attention for each of the heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "id": "tvOvqvX7mGvS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
    "    \n",
    "    assert n_rows * n_cols == n_heads\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,25))\n",
    "    \n",
    "    for i in range(n_heads):\n",
    "        \n",
    "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        \n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        cax = ax.matshow(_attention, cmap='bone')\n",
    "\n",
    "        ax.tick_params(labelsize=12)\n",
    "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
    "                           rotation=45)\n",
    "        ax.set_yticklabels(['']+translation)\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsPrlNylmGvS"
   },
   "source": [
    "First, we'll get an example from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "id": "RvaaKSYamGvS",
    "outputId": "95f2ded3-4ce2-4aa9-f525-3854f8157f8a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['eine', 'frau', 'mit', 'einer', 'großen', 'geldbörse', 'geht', 'an', 'einem', 'tor', 'vorbei', '.']\n",
      "trg = ['a', 'woman', 'with', 'a', 'large', 'purse', 'is', 'walking', 'by', 'a', 'gate', '.']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 8\n",
    "\n",
    "src = vars(train_data.examples[example_idx])['src']\n",
    "trg = vars(train_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMnmyPIHmGvS"
   },
   "source": [
    "Our translation looks pretty good, although our model changes *is walking by* to *walks by*. The meaning is still the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "id": "lMDjEOgTmGvT",
    "outputId": "7fdef3e2-9372-4156-9472-a937bb56df20",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter ##################11######################\n",
      "[0.38868782, 0.38868782]\n",
      "counter ##################11######################\n",
      "[0.23781198, 0.23781198]\n",
      "counter ##################11######################\n",
      "[0.40786085, 0.40786085]\n",
      "counter ##################11######################\n",
      "[nan, nan]\n",
      "counter ##################11######################\n",
      "[nan, nan]\n",
      "counter ##################11######################\n",
      "[nan, nan]\n",
      "counter ##################11######################\n",
      "[nan, nan]\n",
      "counter ##################11######################\n",
      "[nan, nan]\n",
      "counter ##################11######################\n",
      "[nan, nan]\n",
      "counter ##################12######################\n",
      "[1.0000001, 1.0000001]\n",
      "counter ##################12######################\n",
      "[0.99999964, 0.99999964]\n",
      "counter ##################12######################\n",
      "[1.0, 1.0]\n",
      "counter ##################12######################\n",
      "[0.99995583, 0.99995583]\n",
      "counter ##################12######################\n",
      "[1.0, 1.0]\n",
      "counter ##################12######################\n",
      "[0.9929628, 0.9929628]\n",
      "counter ##################13######################\n",
      "[0.9771963, 0.9771963]\n",
      "counter ##################13######################\n",
      "[0.9059206, 0.9059206]\n",
      "counter ##################13######################\n",
      "[0.9418675, 0.9418675]\n",
      "counter ##################13######################\n",
      "[0.995233, 0.995233]\n",
      "counter ##################13######################\n",
      "[0.99889517, 0.99889517]\n",
      "counter ##################13######################\n",
      "[0.8873799, 0.8873799]\n",
      "counter ##################14######################\n",
      "[0.969927, 0.969927]\n",
      "counter ##################14######################\n",
      "[0.8812379, 0.8812379]\n",
      "counter ##################14######################\n",
      "[0.84303975, 0.84303975]\n",
      "counter ##################14######################\n",
      "[0.9912842, 0.9912842]\n",
      "counter ##################14######################\n",
      "[0.9976615, 0.9976615]\n",
      "counter ##################14######################\n",
      "[0.71043724, 0.71043724]\n",
      "counter ##################15######################\n",
      "[0.9706181, 0.9706181]\n",
      "counter ##################15######################\n",
      "[0.86847335, 0.86847335]\n",
      "counter ##################15######################\n",
      "[0.86746186, 0.86746186]\n",
      "counter ##################15######################\n",
      "[0.99141586, 0.99141586]\n",
      "counter ##################15######################\n",
      "[0.99627745, 0.99627745]\n",
      "counter ##################15######################\n",
      "[0.6923803, 0.6923803]\n",
      "counter ##################16######################\n",
      "[0.9711469, 0.9711469]\n",
      "counter ##################16######################\n",
      "[0.7843498, 0.7843498]\n",
      "counter ##################16######################\n",
      "[0.8700469, 0.8700469]\n",
      "counter ##################16######################\n",
      "[0.98771834, 0.98771834]\n",
      "counter ##################16######################\n",
      "[0.9948263, 0.9948263]\n",
      "counter ##################16######################\n",
      "[0.71659595, 0.71659595]\n",
      "counter ##################17######################\n",
      "[0.971196, 0.971196]\n",
      "counter ##################17######################\n",
      "[0.76564246, 0.76564246]\n",
      "counter ##################17######################\n",
      "[0.87146366, 0.87146366]\n",
      "counter ##################17######################\n",
      "[0.9767144, 0.9767144]\n",
      "counter ##################17######################\n",
      "[0.9944497, 0.9944497]\n",
      "counter ##################17######################\n",
      "[0.6291118, 0.6291118]\n",
      "counter ##################18######################\n",
      "[0.9700686, 0.9700686]\n",
      "counter ##################18######################\n",
      "[0.73763186, 0.73763186]\n",
      "counter ##################18######################\n",
      "[0.8672814, 0.8672814]\n",
      "counter ##################18######################\n",
      "[0.95459116, 0.95459116]\n",
      "counter ##################18######################\n",
      "[0.9945437, 0.9945437]\n",
      "counter ##################18######################\n",
      "[0.64439416, 0.64439416]\n",
      "counter ##################19######################\n",
      "[0.96887314, 0.96887314]\n",
      "counter ##################19######################\n",
      "[0.6975222, 0.6975222]\n",
      "counter ##################19######################\n",
      "[0.87975395, 0.87975395]\n",
      "counter ##################19######################\n",
      "[0.9239619, 0.9239619]\n",
      "counter ##################19######################\n",
      "[0.994677, 0.994677]\n",
      "counter ##################19######################\n",
      "[0.6101157, 0.6101157]\n",
      "counter ##################20######################\n",
      "[0.9685723, 0.9685723]\n",
      "counter ##################20######################\n",
      "[0.68483084, 0.68483084]\n",
      "counter ##################20######################\n",
      "[0.8891705, 0.8891705]\n",
      "counter ##################20######################\n",
      "[0.90786, 0.90786]\n",
      "counter ##################20######################\n",
      "[0.9948701, 0.9948701]\n",
      "counter ##################20######################\n",
      "[0.6161342, 0.6161342]\n",
      "counter ##################21######################\n",
      "[0.96736157, 0.96736157]\n",
      "counter ##################21######################\n",
      "[0.66552263, 0.66552263]\n",
      "counter ##################21######################\n",
      "[0.8783613, 0.8783613]\n",
      "counter ##################21######################\n",
      "[0.87063706, 0.87063706]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-418-22506159e639>:99: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  return scaled_hsic / (norm_X * norm_Y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter ##################21######################\n",
      "[0.99511856, 0.99511856]\n",
      "counter ##################21######################\n",
      "[0.5715826, 0.5715826]\n",
      "counter ##################22######################\n",
      "[0.966988, 0.966988]\n",
      "counter ##################22######################\n",
      "[0.67498946, 0.67498946]\n",
      "counter ##################22######################\n",
      "[0.8783673, 0.8783673]\n",
      "counter ##################22######################\n",
      "[0.86134714, 0.86134714]\n",
      "counter ##################22######################\n",
      "[0.99543166, 0.99543166]\n",
      "counter ##################22######################\n",
      "[0.6265624, 0.6265624]\n",
      "counter ##################23######################\n",
      "[0.96649265, 0.96649265]\n",
      "counter ##################23######################\n",
      "[0.66075075, 0.66075075]\n",
      "counter ##################23######################\n",
      "[0.8636967, 0.8636967]\n",
      "counter ##################23######################\n",
      "[0.85748976, 0.85748976]\n",
      "counter ##################23######################\n",
      "[0.9957664, 0.9957664]\n",
      "counter ##################23######################\n",
      "[0.6232785, 0.6232785]\n",
      "counter ##################24######################\n",
      "[0.9655038, 0.9655038]\n",
      "counter ##################24######################\n",
      "[0.6478606, 0.6478606]\n",
      "counter ##################24######################\n",
      "[0.8710089, 0.8710089]\n",
      "counter ##################24######################\n",
      "[0.82291186, 0.82291186]\n",
      "counter ##################24######################\n",
      "[0.9961235, 0.9961235]\n",
      "counter ##################24######################\n",
      "[0.6036067, 0.6036067]\n",
      "counter ##################25######################\n",
      "[0.9651047, 0.9651047]\n",
      "counter ##################25######################\n",
      "[0.63142705, 0.63142705]\n",
      "counter ##################25######################\n",
      "[0.873921, 0.873921]\n",
      "counter ##################25######################\n",
      "[0.8124056, 0.8124056]\n",
      "counter ##################25######################\n",
      "[0.9964223, 0.9964223]\n",
      "counter ##################25######################\n",
      "[0.62784445, 0.62784445]\n",
      "counter ##################26######################\n",
      "[0.964892, 0.964892]\n",
      "counter ##################26######################\n",
      "[0.6232435, 0.6232435]\n",
      "counter ##################26######################\n",
      "[0.87740874, 0.87740874]\n",
      "counter ##################26######################\n",
      "[0.7941027, 0.7941027]\n",
      "counter ##################26######################\n",
      "[0.9966691, 0.9966691]\n",
      "counter ##################26######################\n",
      "[0.6248268, 0.6248268]\n",
      "counter ##################27######################\n",
      "[0.9644531, 0.9644531]\n",
      "counter ##################27######################\n",
      "[0.62510926, 0.62510926]\n",
      "counter ##################27######################\n",
      "[0.8793104, 0.8793104]\n",
      "counter ##################27######################\n",
      "[0.7955819, 0.7955819]\n",
      "counter ##################27######################\n",
      "[0.99689317, 0.99689317]\n",
      "counter ##################27######################\n",
      "[0.5780755, 0.5780755]\n",
      "counter ##################28######################\n",
      "[0.9637694, 0.9637694]\n",
      "counter ##################28######################\n",
      "[0.62227446, 0.62227446]\n",
      "counter ##################28######################\n",
      "[0.87669945, 0.87669945]\n",
      "counter ##################28######################\n",
      "[0.79373616, 0.79373616]\n",
      "counter ##################28######################\n",
      "[0.9970968, 0.9970968]\n",
      "counter ##################28######################\n",
      "[0.5984236, 0.5984236]\n",
      "counter ##################29######################\n",
      "[0.96314245, 0.96314245]\n",
      "counter ##################29######################\n",
      "[0.623659, 0.623659]\n",
      "counter ##################29######################\n",
      "[0.8686281, 0.8686281]\n",
      "counter ##################29######################\n",
      "[0.784104, 0.784104]\n",
      "counter ##################29######################\n",
      "[0.9972645, 0.9972645]\n",
      "counter ##################29######################\n",
      "[0.6252577, 0.6252577]\n",
      "counter ##################30######################\n",
      "[0.96282315, 0.96282315]\n",
      "counter ##################30######################\n",
      "[0.61433405, 0.61433405]\n",
      "counter ##################30######################\n",
      "[0.8740806, 0.8740806]\n",
      "counter ##################30######################\n",
      "[0.784375, 0.784375]\n",
      "counter ##################30######################\n",
      "[0.99739885, 0.99739885]\n",
      "counter ##################30######################\n",
      "[0.60223997, 0.60223997]\n",
      "counter ##################31######################\n",
      "[0.9625448, 0.9625448]\n",
      "counter ##################31######################\n",
      "[0.6126865, 0.6126865]\n",
      "counter ##################31######################\n",
      "[0.8632225, 0.8632225]\n",
      "counter ##################31######################\n",
      "[0.75719297, 0.75719297]\n",
      "counter ##################31######################\n",
      "[0.9975097, 0.9975097]\n",
      "counter ##################31######################\n",
      "[0.5572376, 0.5572376]\n",
      "counter ##################32######################\n",
      "[0.96211934, 0.96211934]\n",
      "counter ##################32######################\n",
      "[0.6012793, 0.6012793]\n",
      "counter ##################32######################\n",
      "[0.8618194, 0.8618194]\n",
      "counter ##################32######################\n",
      "[0.7570871, 0.7570871]\n",
      "counter ##################32######################\n",
      "[0.9976425, 0.9976425]\n",
      "counter ##################32######################\n",
      "[0.574156, 0.574156]\n",
      "counter ##################33######################\n",
      "[0.96168023, 0.96168023]\n",
      "counter ##################33######################\n",
      "[0.5776922, 0.5776922]\n",
      "counter ##################33######################\n",
      "[0.8629728, 0.8629728]\n",
      "counter ##################33######################\n",
      "[0.7535798, 0.7535798]\n",
      "counter ##################33######################\n",
      "[0.99775964, 0.99775964]\n",
      "counter ##################33######################\n",
      "[0.49159116, 0.49159116]\n",
      "counter ##################34######################\n",
      "[0.96115094, 0.96115094]\n",
      "counter ##################34######################\n",
      "[0.55843806, 0.55843806]\n",
      "counter ##################34######################\n",
      "[0.8574088, 0.8574088]\n",
      "counter ##################34######################\n",
      "[0.75261855, 0.75261855]\n",
      "counter ##################34######################\n",
      "[0.9978628, 0.9978628]\n",
      "counter ##################34######################\n",
      "[0.58177114, 0.58177114]\n",
      "counter ##################35######################\n",
      "[0.96079195, 0.96079195]\n",
      "counter ##################35######################\n",
      "[0.5497203, 0.5497203]\n",
      "counter ##################35######################\n",
      "[0.8621704, 0.8621704]\n",
      "counter ##################35######################\n",
      "[0.73013353, 0.73013353]\n",
      "counter ##################35######################\n",
      "[0.9979655, 0.9979655]\n",
      "counter ##################35######################\n",
      "[0.5338061, 0.5338061]\n",
      "counter ##################36######################\n",
      "[0.9602989, 0.9602989]\n",
      "counter ##################36######################\n",
      "[0.54447085, 0.54447085]\n",
      "counter ##################36######################\n",
      "[0.8629147, 0.8629147]\n",
      "counter ##################36######################\n",
      "[0.7302745, 0.7302745]\n",
      "counter ##################36######################\n",
      "[0.9980536, 0.9980536]\n",
      "counter ##################36######################\n",
      "[0.58889323, 0.58889323]\n",
      "counter ##################37######################\n",
      "[0.9598006, 0.9598006]\n",
      "counter ##################37######################\n",
      "[0.5399624, 0.5399624]\n",
      "counter ##################37######################\n",
      "[0.8639996, 0.8639996]\n",
      "counter ##################37######################\n",
      "[0.742668, 0.742668]\n",
      "counter ##################37######################\n",
      "[0.9981452, 0.9981452]\n",
      "counter ##################37######################\n",
      "[0.61530876, 0.61530876]\n",
      "counter ##################38######################\n",
      "[0.9592912, 0.9592912]\n",
      "counter ##################38######################\n",
      "[0.5368255, 0.5368255]\n",
      "counter ##################38######################\n",
      "[0.86467373, 0.86467373]\n",
      "counter ##################38######################\n",
      "[0.7418702, 0.7418702]\n",
      "counter ##################38######################\n",
      "[0.99822617, 0.99822617]\n",
      "counter ##################38######################\n",
      "[0.60752594, 0.60752594]\n",
      "counter ##################39######################\n",
      "[0.95880264, 0.95880264]\n",
      "counter ##################39######################\n",
      "[0.52722543, 0.52722543]\n",
      "counter ##################39######################\n",
      "[0.8684957, 0.8684957]\n",
      "counter ##################39######################\n",
      "[0.74065685, 0.74065685]\n",
      "counter ##################39######################\n",
      "[0.9983038, 0.9983038]\n",
      "counter ##################39######################\n",
      "[0.5935509, 0.5935509]\n",
      "counter ##################40######################\n",
      "[0.9583696, 0.9583696]\n",
      "counter ##################40######################\n",
      "[0.5229585, 0.5229585]\n",
      "counter ##################40######################\n",
      "[0.8677011, 0.8677011]\n",
      "counter ##################40######################\n",
      "[0.74505836, 0.74505836]\n",
      "counter ##################40######################\n",
      "[0.99837255, 0.99837255]\n",
      "counter ##################40######################\n",
      "[0.6077479, 0.6077479]\n",
      "counter ##################41######################\n",
      "[0.9579749, 0.9579749]\n",
      "counter ##################41######################\n",
      "[0.5265338, 0.5265338]\n",
      "counter ##################41######################\n",
      "[0.8687196, 0.8687196]\n",
      "counter ##################41######################\n",
      "[0.7465536, 0.7465536]\n",
      "counter ##################41######################\n",
      "[0.9984315, 0.9984315]\n",
      "counter ##################41######################\n",
      "[0.5856727, 0.5856727]\n",
      "counter ##################42######################\n",
      "[0.9576043, 0.9576043]\n",
      "counter ##################42######################\n",
      "[0.52271956, 0.52271956]\n",
      "counter ##################42######################\n",
      "[0.86821514, 0.86821514]\n",
      "counter ##################42######################\n",
      "[0.7473239, 0.7473239]\n",
      "counter ##################42######################\n",
      "[0.9984915, 0.9984915]\n",
      "counter ##################42######################\n",
      "[0.59623235, 0.59623235]\n",
      "counter ##################43######################\n",
      "[0.95717084, 0.95717084]\n",
      "counter ##################43######################\n",
      "[0.5197011, 0.5197011]\n",
      "counter ##################43######################\n",
      "[0.87093776, 0.87093776]\n",
      "counter ##################43######################\n",
      "[0.7315147, 0.7315147]\n",
      "counter ##################43######################\n",
      "[0.9985453, 0.9985453]\n",
      "counter ##################43######################\n",
      "[0.5711819, 0.5711819]\n",
      "counter ##################44######################\n",
      "[0.9567385, 0.9567385]\n",
      "counter ##################44######################\n",
      "[0.51197267, 0.51197267]\n",
      "counter ##################44######################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8731117, 0.8731117]\n",
      "counter ##################44######################\n",
      "[0.7203921, 0.7203921]\n",
      "counter ##################44######################\n",
      "[0.9985954, 0.9985954]\n",
      "counter ##################44######################\n",
      "[0.5921751, 0.5921751]\n",
      "counter ##################45######################\n",
      "[0.95629376, 0.95629376]\n",
      "counter ##################45######################\n",
      "[0.508192, 0.508192]\n",
      "counter ##################45######################\n",
      "[0.87599367, 0.87599367]\n",
      "counter ##################45######################\n",
      "[0.72187793, 0.72187793]\n",
      "counter ##################45######################\n",
      "[0.9986443, 0.9986443]\n",
      "counter ##################45######################\n",
      "[0.5785764, 0.5785764]\n",
      "counter ##################46######################\n",
      "[0.9559967, 0.9559967]\n",
      "counter ##################46######################\n",
      "[0.49997297, 0.49997297]\n",
      "counter ##################46######################\n",
      "[0.8758766, 0.8758766]\n",
      "counter ##################46######################\n",
      "[0.7190387, 0.7190387]\n",
      "counter ##################46######################\n",
      "[0.9986899, 0.9986899]\n",
      "counter ##################46######################\n",
      "[0.5229748, 0.5229748]\n",
      "counter ##################47######################\n",
      "[0.95573497, 0.95573497]\n",
      "counter ##################47######################\n",
      "[0.501648, 0.501648]\n",
      "counter ##################47######################\n",
      "[0.8768166, 0.8768166]\n",
      "counter ##################47######################\n",
      "[0.724836, 0.724836]\n",
      "counter ##################47######################\n",
      "[0.99873334, 0.99873334]\n",
      "counter ##################47######################\n",
      "[0.56135803, 0.56135803]\n",
      "counter ##################48######################\n",
      "[0.9553225, 0.9553225]\n",
      "counter ##################48######################\n",
      "[0.496079, 0.496079]\n",
      "counter ##################48######################\n",
      "[0.8790596, 0.8790596]\n",
      "counter ##################48######################\n",
      "[0.72477835, 0.72477835]\n",
      "counter ##################48######################\n",
      "[0.9987722, 0.9987722]\n",
      "counter ##################48######################\n",
      "[0.5496753, 0.5496753]\n",
      "counter ##################49######################\n",
      "[0.95496154, 0.95496154]\n",
      "counter ##################49######################\n",
      "[0.4945731, 0.4945731]\n",
      "counter ##################49######################\n",
      "[0.87904155, 0.87904155]\n",
      "counter ##################49######################\n",
      "[0.71532804, 0.71532804]\n",
      "counter ##################49######################\n",
      "[0.99880844, 0.99880844]\n",
      "counter ##################49######################\n",
      "[0.5542762, 0.5542762]\n",
      "counter ##################50######################\n",
      "[0.95465684, 0.95465684]\n",
      "counter ##################50######################\n",
      "[0.48574734, 0.48574734]\n",
      "counter ##################50######################\n",
      "[0.8812006, 0.8812006]\n",
      "counter ##################50######################\n",
      "[0.71042657, 0.71042657]\n",
      "counter ##################50######################\n",
      "[0.99884224, 0.99884224]\n",
      "counter ##################50######################\n",
      "[0.5477736, 0.5477736]\n",
      "counter ##################51######################\n",
      "[0.9543952, 0.9543952]\n",
      "counter ##################51######################\n",
      "[0.48260164, 0.48260164]\n",
      "counter ##################51######################\n",
      "[0.8801129, 0.8801129]\n",
      "counter ##################51######################\n",
      "[0.7143885, 0.7143885]\n",
      "counter ##################51######################\n",
      "[0.9988698, 0.9988698]\n",
      "counter ##################51######################\n",
      "[0.554814, 0.554814]\n",
      "counter ##################52######################\n",
      "[0.9541619, 0.9541619]\n",
      "counter ##################52######################\n",
      "[0.48067006, 0.48067006]\n",
      "counter ##################52######################\n",
      "[0.87999886, 0.87999886]\n",
      "counter ##################52######################\n",
      "[0.72044, 0.72044]\n",
      "counter ##################52######################\n",
      "[0.9988943, 0.9988943]\n",
      "counter ##################52######################\n",
      "[0.5367371, 0.5367371]\n",
      "counter ##################53######################\n",
      "[0.9539312, 0.9539312]\n",
      "counter ##################53######################\n",
      "[0.47800016, 0.47800016]\n",
      "counter ##################53######################\n",
      "[0.8803183, 0.8803183]\n",
      "counter ##################53######################\n",
      "[0.7224518, 0.7224518]\n",
      "counter ##################53######################\n",
      "[0.9989188, 0.9989188]\n",
      "counter ##################53######################\n",
      "[0.5515153, 0.5515153]\n",
      "counter ##################54######################\n",
      "[0.9536831, 0.9536831]\n",
      "counter ##################54######################\n",
      "[0.46240658, 0.46240658]\n",
      "counter ##################54######################\n",
      "[0.8801576, 0.8801576]\n",
      "counter ##################54######################\n",
      "[0.7261047, 0.7261047]\n",
      "counter ##################54######################\n",
      "[0.9989423, 0.9989423]\n",
      "counter ##################54######################\n",
      "[0.5373814, 0.5373814]\n",
      "counter ##################55######################\n",
      "[0.9534302, 0.9534302]\n",
      "counter ##################55######################\n",
      "[0.4456714, 0.4456714]\n",
      "counter ##################55######################\n",
      "[0.8807, 0.8807]\n",
      "counter ##################55######################\n",
      "[0.72838545, 0.72838545]\n",
      "counter ##################55######################\n",
      "[0.9989637, 0.9989637]\n",
      "counter ##################55######################\n",
      "[0.53343105, 0.53343105]\n",
      "counter ##################56######################\n",
      "[0.9532269, 0.9532269]\n",
      "counter ##################56######################\n",
      "[0.4373742, 0.4373742]\n",
      "counter ##################56######################\n",
      "[0.88193333, 0.88193333]\n",
      "counter ##################56######################\n",
      "[0.7278108, 0.7278108]\n",
      "counter ##################56######################\n",
      "[0.9989866, 0.9989866]\n",
      "counter ##################56######################\n",
      "[0.51583725, 0.51583725]\n",
      "counter ##################57######################\n",
      "[0.9529429, 0.9529429]\n",
      "counter ##################57######################\n",
      "[0.43653378, 0.43653378]\n",
      "counter ##################57######################\n",
      "[0.88086796, 0.88086796]\n",
      "counter ##################57######################\n",
      "[0.7340499, 0.7340499]\n",
      "counter ##################57######################\n",
      "[0.99900687, 0.99900687]\n",
      "counter ##################57######################\n",
      "[0.5212043, 0.5212043]\n",
      "counter ##################58######################\n",
      "[0.9527947, 0.9527947]\n",
      "counter ##################58######################\n",
      "[0.43562382, 0.43562382]\n",
      "counter ##################58######################\n",
      "[0.88068694, 0.88068694]\n",
      "counter ##################58######################\n",
      "[0.7323528, 0.7323528]\n",
      "counter ##################58######################\n",
      "[0.99902594, 0.99902594]\n",
      "counter ##################58######################\n",
      "[0.5055678, 0.5055678]\n",
      "counter ##################59######################\n",
      "[0.9526501, 0.9526501]\n",
      "counter ##################59######################\n",
      "[0.4345491, 0.4345491]\n",
      "counter ##################59######################\n",
      "[0.8799712, 0.8799712]\n",
      "counter ##################59######################\n",
      "[0.73718894, 0.73718894]\n",
      "counter ##################59######################\n",
      "[0.99904233, 0.99904233]\n",
      "counter ##################59######################\n",
      "[0.5080671, 0.5080671]\n",
      "counter ##################60######################\n",
      "[0.952529, 0.952529]\n",
      "counter ##################60######################\n",
      "[0.4334603, 0.4334603]\n",
      "counter ##################60######################\n",
      "[0.8804031, 0.8804031]\n",
      "counter ##################60######################\n",
      "[0.7387477, 0.7387477]\n",
      "counter ##################60######################\n",
      "[0.99905664, 0.99905664]\n",
      "counter ##################60######################\n",
      "[0.52364165, 0.52364165]\n",
      "predicted trg = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufACoCrUmGvT"
   },
   "source": [
    "We can see the attention from each head below. Each is certainly different, but it's difficult (perhaps impossible) to reason about what head has actually learned to pay attention to. Some heads pay full attention to \"eine\" when translating \"a\", some don't at all, and some do a little. They all seem to follow the similar \"downward staircase\" pattern and the attention when outputting the last two tokens is equally spread over the final two tokens in the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "id": "3OhNKxeEmGvT",
    "outputId": "693d5c70-b20b-4b58-82a2-113202456a9e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9ylfV0ZmGvT"
   },
   "source": [
    "Next, let's get an example the model has not been trained on from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "id": "ua2kEgZHmGvT",
    "outputId": "9f47ead4-4d11-442c-8060-db8277f6bed7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['ein', 'brauner', 'hund', 'rennt', 'dem', 'schwarzen', 'hund', 'hinterher', '.']\n",
      "trg = ['a', 'brown', 'dog', 'is', 'running', 'after', 'the', 'black', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 6\n",
    "\n",
    "src = vars(valid_data.examples[example_idx])['src']\n",
    "trg = vars(valid_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mzQAaSGmGvT"
   },
   "source": [
    "The model translates it by switching *is running* to just *runs*, but it is an acceptable swap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "id": "3w8-0S3lmGvU",
    "outputId": "1c36e3f1-b80d-492a-9b75-2fa6f913ec96",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter ##################12######################\n",
      "[0.25472116, 0.25472116]\n",
      "counter ##################12######################\n",
      "[0.27316993, 0.27316993]\n",
      "counter ##################12######################\n",
      "[0.5065412, 0.5065412]\n",
      "counter ##################61######################\n",
      "[nan, nan]\n",
      "counter ##################61######################\n",
      "[nan, nan]\n",
      "counter ##################61######################\n",
      "[nan, nan]\n",
      "counter ##################61######################\n",
      "[nan, nan]\n",
      "counter ##################61######################\n",
      "[nan, nan]\n",
      "counter ##################61######################\n",
      "[nan, nan]\n",
      "counter ##################62######################\n",
      "[1.0000001, 1.0000001]\n",
      "counter ##################62######################\n",
      "[0.9999999, 0.9999999]\n",
      "counter ##################62######################\n",
      "[1.0, 1.0]\n",
      "counter ##################62######################\n",
      "[0.99999964, 0.99999964]\n",
      "counter ##################62######################\n",
      "[1.0, 1.0]\n",
      "counter ##################62######################\n",
      "[0.83862793, 0.83862793]\n",
      "counter ##################63######################\n",
      "[0.9771963, 0.9771963]\n",
      "counter ##################63######################\n",
      "[0.99705017, 0.99705017]\n",
      "counter ##################63######################\n",
      "[0.9562696, 0.9562696]\n",
      "counter ##################63######################\n",
      "[0.9877618, 0.9877618]\n",
      "counter ##################63######################\n",
      "[0.9992056, 0.9992056]\n",
      "counter ##################63######################\n",
      "[0.80782187, 0.80782187]\n",
      "counter ##################64######################\n",
      "[0.969927, 0.969927]\n",
      "counter ##################64######################\n",
      "[0.9443179, 0.9443179]\n",
      "counter ##################64######################\n",
      "[0.8726305, 0.8726305]\n",
      "counter ##################64######################\n",
      "[0.98574895, 0.98574895]\n",
      "counter ##################64######################\n",
      "[0.99858546, 0.99858546]\n",
      "counter ##################64######################\n",
      "[0.5908833, 0.5908833]\n",
      "counter ##################65######################\n",
      "[0.9706181, 0.9706181]\n",
      "counter ##################65######################\n",
      "[0.95161515, 0.95161515]\n",
      "counter ##################65######################\n",
      "[0.8928611, 0.8928611]\n",
      "counter ##################65######################\n",
      "[0.9776263, 0.9776263]\n",
      "counter ##################65######################\n",
      "[0.99779797, 0.99779797]\n",
      "counter ##################65######################\n",
      "[0.5998969, 0.5998969]\n",
      "counter ##################66######################\n",
      "[0.9711469, 0.9711469]\n",
      "counter ##################66######################\n",
      "[0.95477074, 0.95477074]\n",
      "counter ##################66######################\n",
      "[0.9000748, 0.9000748]\n",
      "counter ##################66######################\n",
      "[0.9799454, 0.9799454]\n",
      "counter ##################66######################\n",
      "[0.9967839, 0.9967839]\n",
      "counter ##################66######################\n",
      "[0.62991786, 0.62991786]\n",
      "counter ##################67######################\n",
      "[0.971196, 0.971196]\n",
      "counter ##################67######################\n",
      "[0.95523846, 0.95523846]\n",
      "counter ##################67######################\n",
      "[0.8960563, 0.8960563]\n",
      "counter ##################67######################\n",
      "[0.9721765, 0.9721765]\n",
      "counter ##################67######################\n",
      "[0.9964305, 0.9964305]\n",
      "counter ##################67######################\n",
      "[0.6258245, 0.6258245]\n",
      "counter ##################68######################\n",
      "[0.9700686, 0.9700686]\n",
      "counter ##################68######################\n",
      "[0.94718915, 0.94718915]\n",
      "counter ##################68######################\n",
      "[0.8859387, 0.8859387]\n",
      "counter ##################68######################\n",
      "[0.9570497, 0.9570497]\n",
      "counter ##################68######################\n",
      "[0.99650115, 0.99650115]\n",
      "counter ##################68######################\n",
      "[0.6933187, 0.6933187]\n",
      "counter ##################69######################\n",
      "[0.96887314, 0.96887314]\n",
      "counter ##################69######################\n",
      "[0.93510365, 0.93510365]\n",
      "counter ##################69######################\n",
      "[0.8961547, 0.8961547]\n",
      "counter ##################69######################\n",
      "[0.9517689, 0.9517689]\n",
      "counter ##################69######################\n",
      "[0.99660075, 0.99660075]\n",
      "counter ##################69######################\n",
      "[0.69769824, 0.69769824]\n",
      "counter ##################70######################\n",
      "[0.9685723, 0.9685723]\n",
      "counter ##################70######################\n",
      "[0.92779994, 0.92779994]\n",
      "counter ##################70######################\n",
      "[0.90206134, 0.90206134]\n",
      "counter ##################70######################\n",
      "[0.94964004, 0.94964004]\n",
      "counter ##################70######################\n",
      "[0.9967246, 0.9967246]\n",
      "counter ##################70######################\n",
      "[0.65353674, 0.65353674]\n",
      "counter ##################71######################\n",
      "[0.96736157, 0.96736157]\n",
      "counter ##################71######################\n",
      "[0.92563534, 0.92563534]\n",
      "counter ##################71######################\n",
      "[0.88590765, 0.88590765]\n",
      "counter ##################71######################\n",
      "[0.9310939, 0.9310939]\n",
      "counter ##################71######################\n",
      "[0.99688315, 0.99688315]\n",
      "counter ##################71######################\n",
      "[0.59806854, 0.59806854]\n",
      "counter ##################72######################\n",
      "[0.966988, 0.966988]\n",
      "counter ##################72######################\n",
      "[0.9282251, 0.9282251]\n",
      "counter ##################72######################\n",
      "[0.88257664, 0.88257664]\n",
      "counter ##################72######################\n",
      "[0.935797, 0.935797]\n",
      "counter ##################72######################\n",
      "[0.99710304, 0.99710304]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-418-22506159e639>:99: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  return scaled_hsic / (norm_X * norm_Y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter ##################72######################\n",
      "[0.64161885, 0.64161885]\n",
      "counter ##################73######################\n",
      "[0.96649265, 0.96649265]\n",
      "counter ##################73######################\n",
      "[0.9255552, 0.9255552]\n",
      "counter ##################73######################\n",
      "[0.86871374, 0.86871374]\n",
      "counter ##################73######################\n",
      "[0.92425555, 0.92425555]\n",
      "counter ##################73######################\n",
      "[0.99734175, 0.99734175]\n",
      "counter ##################73######################\n",
      "[0.6363481, 0.6363481]\n",
      "counter ##################74######################\n",
      "[0.9655038, 0.9655038]\n",
      "counter ##################74######################\n",
      "[0.92618436, 0.92618436]\n",
      "counter ##################74######################\n",
      "[0.8752901, 0.8752901]\n",
      "counter ##################74######################\n",
      "[0.91851336, 0.91851336]\n",
      "counter ##################74######################\n",
      "[0.99757355, 0.99757355]\n",
      "counter ##################74######################\n",
      "[0.5445714, 0.5445714]\n",
      "counter ##################75######################\n",
      "[0.9651047, 0.9651047]\n",
      "counter ##################75######################\n",
      "[0.92650557, 0.92650557]\n",
      "counter ##################75######################\n",
      "[0.8796516, 0.8796516]\n",
      "counter ##################75######################\n",
      "[0.8994884, 0.8994884]\n",
      "counter ##################75######################\n",
      "[0.997765, 0.997765]\n",
      "counter ##################75######################\n",
      "[0.6409723, 0.6409723]\n",
      "counter ##################76######################\n",
      "[0.964892, 0.964892]\n",
      "counter ##################76######################\n",
      "[0.92762214, 0.92762214]\n",
      "counter ##################76######################\n",
      "[0.88228506, 0.88228506]\n",
      "counter ##################76######################\n",
      "[0.8916844, 0.8916844]\n",
      "counter ##################76######################\n",
      "[0.9979225, 0.9979225]\n",
      "counter ##################76######################\n",
      "[0.63252616, 0.63252616]\n",
      "counter ##################77######################\n",
      "[0.9644531, 0.9644531]\n",
      "counter ##################77######################\n",
      "[0.92796916, 0.92796916]\n",
      "counter ##################77######################\n",
      "[0.8835261, 0.8835261]\n",
      "counter ##################77######################\n",
      "[0.8884355, 0.8884355]\n",
      "counter ##################77######################\n",
      "[0.9980602, 0.9980602]\n",
      "counter ##################77######################\n",
      "[0.6037139, 0.6037139]\n",
      "counter ##################78######################\n",
      "[0.9637694, 0.9637694]\n",
      "counter ##################78######################\n",
      "[0.9257176, 0.9257176]\n",
      "counter ##################78######################\n",
      "[0.88132656, 0.88132656]\n",
      "counter ##################78######################\n",
      "[0.88594496, 0.88594496]\n",
      "counter ##################78######################\n",
      "[0.99818426, 0.99818426]\n",
      "counter ##################78######################\n",
      "[0.5277776, 0.5277776]\n",
      "counter ##################79######################\n",
      "[0.96314245, 0.96314245]\n",
      "counter ##################79######################\n",
      "[0.9216871, 0.9216871]\n",
      "counter ##################79######################\n",
      "[0.8749745, 0.8749745]\n",
      "counter ##################79######################\n",
      "[0.8832459, 0.8832459]\n",
      "counter ##################79######################\n",
      "[0.9982893, 0.9982893]\n",
      "counter ##################79######################\n",
      "[0.56712973, 0.56712973]\n",
      "counter ##################80######################\n",
      "[0.96282315, 0.96282315]\n",
      "counter ##################80######################\n",
      "[0.9197976, 0.9197976]\n",
      "counter ##################80######################\n",
      "[0.8803053, 0.8803053]\n",
      "counter ##################80######################\n",
      "[0.88346386, 0.88346386]\n",
      "counter ##################80######################\n",
      "[0.9983711, 0.9983711]\n",
      "counter ##################80######################\n",
      "[0.57643116, 0.57643116]\n",
      "counter ##################81######################\n",
      "[0.9625448, 0.9625448]\n",
      "counter ##################81######################\n",
      "[0.9203836, 0.9203836]\n",
      "counter ##################81######################\n",
      "[0.8706156, 0.8706156]\n",
      "counter ##################81######################\n",
      "[0.87855786, 0.87855786]\n",
      "counter ##################81######################\n",
      "[0.99844086, 0.99844086]\n",
      "counter ##################81######################\n",
      "[0.5260083, 0.5260083]\n",
      "counter ##################82######################\n",
      "[0.96211934, 0.96211934]\n",
      "counter ##################82######################\n",
      "[0.8876654, 0.8876654]\n",
      "counter ##################82######################\n",
      "[0.8689122, 0.8689122]\n",
      "counter ##################82######################\n",
      "[0.8715411, 0.8715411]\n",
      "counter ##################82######################\n",
      "[0.99852324, 0.99852324]\n",
      "counter ##################82######################\n",
      "[0.49474147, 0.49474147]\n",
      "counter ##################83######################\n",
      "[0.96168023, 0.96168023]\n",
      "counter ##################83######################\n",
      "[0.8872746, 0.8872746]\n",
      "counter ##################83######################\n",
      "[0.86944157, 0.86944157]\n",
      "counter ##################83######################\n",
      "[0.86223555, 0.86223555]\n",
      "counter ##################83######################\n",
      "[0.9985931, 0.9985931]\n",
      "counter ##################83######################\n",
      "[0.4988587, 0.4988587]\n",
      "counter ##################84######################\n",
      "[0.96115094, 0.96115094]\n",
      "counter ##################84######################\n",
      "[0.88426363, 0.88426363]\n",
      "counter ##################84######################\n",
      "[0.8623799, 0.8623799]\n",
      "counter ##################84######################\n",
      "[0.84256655, 0.84256655]\n",
      "counter ##################84######################\n",
      "[0.9986545, 0.9986545]\n",
      "counter ##################84######################\n",
      "[0.43951228, 0.43951228]\n",
      "counter ##################85######################\n",
      "[0.96079195, 0.96079195]\n",
      "counter ##################85######################\n",
      "[0.8840245, 0.8840245]\n",
      "counter ##################85######################\n",
      "[0.86693907, 0.86693907]\n",
      "counter ##################85######################\n",
      "[0.7986809, 0.7986809]\n",
      "counter ##################85######################\n",
      "[0.99871576, 0.99871576]\n",
      "counter ##################85######################\n",
      "[0.4196826, 0.4196826]\n",
      "counter ##################86######################\n",
      "[0.9602989, 0.9602989]\n",
      "counter ##################86######################\n",
      "[0.8783133, 0.8783133]\n",
      "counter ##################86######################\n",
      "[0.86755186, 0.86755186]\n",
      "counter ##################86######################\n",
      "[0.80611086, 0.80611086]\n",
      "counter ##################86######################\n",
      "[0.99876696, 0.99876696]\n",
      "counter ##################86######################\n",
      "[0.47496828, 0.47496828]\n",
      "counter ##################87######################\n",
      "[0.9598006, 0.9598006]\n",
      "counter ##################87######################\n",
      "[0.8654378, 0.8654378]\n",
      "counter ##################87######################\n",
      "[0.8682987, 0.8682987]\n",
      "counter ##################87######################\n",
      "[0.7808584, 0.7808584]\n",
      "counter ##################87######################\n",
      "[0.99882036, 0.99882036]\n",
      "counter ##################87######################\n",
      "[0.4608857, 0.4608857]\n",
      "counter ##################88######################\n",
      "[0.9592912, 0.9592912]\n",
      "counter ##################88######################\n",
      "[0.86637545, 0.86637545]\n",
      "counter ##################88######################\n",
      "[0.8682447, 0.8682447]\n",
      "counter ##################88######################\n",
      "[0.77012324, 0.77012324]\n",
      "counter ##################88######################\n",
      "[0.9988682, 0.9988682]\n",
      "counter ##################88######################\n",
      "[0.44052258, 0.44052258]\n",
      "counter ##################89######################\n",
      "[0.95880264, 0.95880264]\n",
      "counter ##################89######################\n",
      "[0.86632836, 0.86632836]\n",
      "counter ##################89######################\n",
      "[0.87205106, 0.87205106]\n",
      "counter ##################89######################\n",
      "[0.7739784, 0.7739784]\n",
      "counter ##################89######################\n",
      "[0.9989137, 0.9989137]\n",
      "counter ##################89######################\n",
      "[0.45050573, 0.45050573]\n",
      "counter ##################90######################\n",
      "[0.9583696, 0.9583696]\n",
      "counter ##################90######################\n",
      "[0.85729533, 0.85729533]\n",
      "counter ##################90######################\n",
      "[0.8708546, 0.8708546]\n",
      "counter ##################90######################\n",
      "[0.7816773, 0.7816773]\n",
      "counter ##################90######################\n",
      "[0.9989545, 0.9989545]\n",
      "counter ##################90######################\n",
      "[0.32114834, 0.32114834]\n",
      "counter ##################91######################\n",
      "[0.9579749, 0.9579749]\n",
      "counter ##################91######################\n",
      "[0.84881574, 0.84881574]\n",
      "counter ##################91######################\n",
      "[0.87097967, 0.87097967]\n",
      "counter ##################91######################\n",
      "[0.78948987, 0.78948987]\n",
      "counter ##################91######################\n",
      "[0.9989884, 0.9989884]\n",
      "counter ##################91######################\n",
      "[0.45451963, 0.45451963]\n",
      "counter ##################92######################\n",
      "[0.9576043, 0.9576043]\n",
      "counter ##################92######################\n",
      "[0.84235096, 0.84235096]\n",
      "counter ##################92######################\n",
      "[0.86919045, 0.86919045]\n",
      "counter ##################92######################\n",
      "[0.7929199, 0.7929199]\n",
      "counter ##################92######################\n",
      "[0.9990225, 0.9990225]\n",
      "counter ##################92######################\n",
      "[0.4528801, 0.4528801]\n",
      "counter ##################93######################\n",
      "[0.95717084, 0.95717084]\n",
      "counter ##################93######################\n",
      "[0.8293514, 0.8293514]\n",
      "counter ##################93######################\n",
      "[0.8719404, 0.8719404]\n",
      "counter ##################93######################\n",
      "[0.7966269, 0.7966269]\n",
      "counter ##################93######################\n",
      "[0.9990539, 0.9990539]\n",
      "counter ##################93######################\n",
      "[0.48305666, 0.48305666]\n",
      "counter ##################94######################\n",
      "[0.9567385, 0.9567385]\n",
      "counter ##################94######################\n",
      "[0.82413596, 0.82413596]\n",
      "counter ##################94######################\n",
      "[0.873727, 0.873727]\n",
      "counter ##################94######################\n",
      "[0.79771703, 0.79771703]\n",
      "counter ##################94######################\n",
      "[0.9990831, 0.9990831]\n",
      "counter ##################94######################\n",
      "[0.50236803, 0.50236803]\n",
      "counter ##################95######################\n",
      "[0.95629376, 0.95629376]\n",
      "counter ##################95######################\n",
      "[0.8224364, 0.8224364]\n",
      "counter ##################95######################\n",
      "[0.8765024, 0.8765024]\n",
      "counter ##################95######################\n",
      "[0.78552806, 0.78552806]\n",
      "counter ##################95######################\n",
      "[0.99911195, 0.99911195]\n",
      "counter ##################95######################\n",
      "[0.48882648, 0.48882648]\n",
      "counter ##################96######################\n",
      "[0.9559967, 0.9559967]\n",
      "counter ##################96######################\n",
      "[0.82200456, 0.82200456]\n",
      "counter ##################96######################\n",
      "[0.8758154, 0.8758154]\n",
      "counter ##################96######################\n",
      "[0.78802013, 0.78802013]\n",
      "counter ##################96######################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9991389, 0.9991389]\n",
      "counter ##################96######################\n",
      "[0.42847818, 0.42847818]\n",
      "counter ##################97######################\n",
      "[0.95573497, 0.95573497]\n",
      "counter ##################97######################\n",
      "[0.82149154, 0.82149154]\n",
      "counter ##################97######################\n",
      "[0.87672865, 0.87672865]\n",
      "counter ##################97######################\n",
      "[0.78816116, 0.78816116]\n",
      "counter ##################97######################\n",
      "[0.99916446, 0.99916446]\n",
      "counter ##################97######################\n",
      "[0.46423626, 0.46423626]\n",
      "counter ##################98######################\n",
      "[0.9553225, 0.9553225]\n",
      "counter ##################98######################\n",
      "[0.81860787, 0.81860787]\n",
      "counter ##################98######################\n",
      "[0.8789, 0.8789]\n",
      "counter ##################98######################\n",
      "[0.7899219, 0.7899219]\n",
      "counter ##################98######################\n",
      "[0.9991862, 0.9991862]\n",
      "counter ##################98######################\n",
      "[0.47024232, 0.47024232]\n",
      "counter ##################99######################\n",
      "[0.95496154, 0.95496154]\n",
      "counter ##################99######################\n",
      "[0.8178751, 0.8178751]\n",
      "counter ##################99######################\n",
      "[0.8779586, 0.8779586]\n",
      "counter ##################99######################\n",
      "[0.7924266, 0.7924266]\n",
      "counter ##################99######################\n",
      "[0.99920696, 0.99920696]\n",
      "counter ##################99######################\n",
      "[0.47186223, 0.47186223]\n",
      "counter ##################100######################\n",
      "[0.95465684, 0.95465684]\n",
      "counter ##################100######################\n",
      "[0.81399035, 0.81399035]\n",
      "counter ##################100######################\n",
      "[0.88041097, 0.88041097]\n",
      "counter ##################100######################\n",
      "[0.7700305, 0.7700305]\n",
      "counter ##################100######################\n",
      "[0.99922633, 0.99922633]\n",
      "counter ##################100######################\n",
      "[0.4655435, 0.4655435]\n",
      "counter ##################101######################\n",
      "[0.9543952, 0.9543952]\n",
      "counter ##################101######################\n",
      "[0.8111132, 0.8111132]\n",
      "counter ##################101######################\n",
      "[0.8789009, 0.8789009]\n",
      "counter ##################101######################\n",
      "[0.7681462, 0.7681462]\n",
      "counter ##################101######################\n",
      "[0.9992407, 0.9992407]\n",
      "counter ##################101######################\n",
      "[0.40338632, 0.40338632]\n",
      "counter ##################102######################\n",
      "[0.9541619, 0.9541619]\n",
      "counter ##################102######################\n",
      "[0.80956256, 0.80956256]\n",
      "counter ##################102######################\n",
      "[0.8782635, 0.8782635]\n",
      "counter ##################102######################\n",
      "[0.7668511, 0.7668511]\n",
      "counter ##################102######################\n",
      "[0.99925286, 0.99925286]\n",
      "counter ##################102######################\n",
      "[0.3838593, 0.3838593]\n",
      "counter ##################103######################\n",
      "[0.9539312, 0.9539312]\n",
      "counter ##################103######################\n",
      "[0.8089259, 0.8089259]\n",
      "counter ##################103######################\n",
      "[0.8786603, 0.8786603]\n",
      "counter ##################103######################\n",
      "[0.7625272, 0.7625272]\n",
      "counter ##################103######################\n",
      "[0.999266, 0.999266]\n",
      "counter ##################103######################\n",
      "[0.3995014, 0.3995014]\n",
      "counter ##################104######################\n",
      "[0.9536831, 0.9536831]\n",
      "counter ##################104######################\n",
      "[0.8077735, 0.8077735]\n",
      "counter ##################104######################\n",
      "[0.8775418, 0.8775418]\n",
      "counter ##################104######################\n",
      "[0.7562205, 0.7562205]\n",
      "counter ##################104######################\n",
      "[0.9992788, 0.9992788]\n",
      "counter ##################104######################\n",
      "[0.4137093, 0.4137093]\n",
      "counter ##################105######################\n",
      "[0.9534302, 0.9534302]\n",
      "counter ##################105######################\n",
      "[0.8010518, 0.8010518]\n",
      "counter ##################105######################\n",
      "[0.8777431, 0.8777431]\n",
      "counter ##################105######################\n",
      "[0.75996166, 0.75996166]\n",
      "counter ##################105######################\n",
      "[0.9992904, 0.9992904]\n",
      "counter ##################105######################\n",
      "[0.36318672, 0.36318672]\n",
      "counter ##################106######################\n",
      "[0.9532269, 0.9532269]\n",
      "counter ##################106######################\n",
      "[0.8006146, 0.8006146]\n",
      "counter ##################106######################\n",
      "[0.8786746, 0.8786746]\n",
      "counter ##################106######################\n",
      "[0.7624713, 0.7624713]\n",
      "counter ##################106######################\n",
      "[0.99930364, 0.99930364]\n",
      "counter ##################106######################\n",
      "[0.37693855, 0.37693855]\n",
      "counter ##################107######################\n",
      "[0.9529429, 0.9529429]\n",
      "counter ##################107######################\n",
      "[0.7996321, 0.7996321]\n",
      "counter ##################107######################\n",
      "[0.87711614, 0.87711614]\n",
      "counter ##################107######################\n",
      "[0.7649929, 0.7649929]\n",
      "counter ##################107######################\n",
      "[0.9993163, 0.9993163]\n",
      "counter ##################107######################\n",
      "[0.385617, 0.385617]\n",
      "counter ##################108######################\n",
      "[0.9527947, 0.9527947]\n",
      "counter ##################108######################\n",
      "[0.7981119, 0.7981119]\n",
      "counter ##################108######################\n",
      "[0.8768488, 0.8768488]\n",
      "counter ##################108######################\n",
      "[0.767176, 0.767176]\n",
      "counter ##################108######################\n",
      "[0.99932784, 0.99932784]\n",
      "counter ##################108######################\n",
      "[0.33361623, 0.33361623]\n",
      "counter ##################109######################\n",
      "[0.9526501, 0.9526501]\n",
      "counter ##################109######################\n",
      "[0.7716896, 0.7716896]\n",
      "counter ##################109######################\n",
      "[0.8753432, 0.8753432]\n",
      "counter ##################109######################\n",
      "[0.76892424, 0.76892424]\n",
      "counter ##################109######################\n",
      "[0.9993378, 0.9993378]\n",
      "counter ##################109######################\n",
      "[0.38557953, 0.38557953]\n",
      "counter ##################110######################\n",
      "[0.952529, 0.952529]\n",
      "counter ##################110######################\n",
      "[0.771383, 0.771383]\n",
      "counter ##################110######################\n",
      "[0.87508476, 0.87508476]\n",
      "counter ##################110######################\n",
      "[0.7698311, 0.7698311]\n",
      "counter ##################110######################\n",
      "[0.9993438, 0.9993438]\n",
      "counter ##################110######################\n",
      "[0.3890803, 0.3890803]\n",
      "predicted trg = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tk_J-ARRmGvU"
   },
   "source": [
    "Again, some heads pay full attention to \"ein\" whilst some pay no attention to it. Again, most of the heads seem to spread their attention over both the period and `<eos>` tokens in the source sentence when outputting the period and `<eos>` sentence in the predicted target sentence, though some seem to pay attention to tokens from near the start of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "id": "Q24ETTacmGvU",
    "outputId": "75c815c7-9dc7-4af1-cbb1-8c5788b0c152",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CPwnjALmGvU"
   },
   "source": [
    "Finally, we'll look at an example from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "id": "PDPBQHJQmGvU",
    "outputId": "65ae10a6-2809-4ceb-9732-17a8ecba7512",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.']\n",
      "trg = ['a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 10\n",
    "\n",
    "src = vars(test_data.examples[example_idx])['src']\n",
    "trg = vars(test_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97NvZR4smGvU"
   },
   "source": [
    "A perfect translation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "id": "fcrA4TpomGvV",
    "outputId": "7db0d5db-465d-48ac-d5fb-107fefd3f45c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter ##################13######################\n",
      "[0.27688786, 0.27688786]\n",
      "counter ##################13######################\n",
      "[0.5178543, 0.5178543]\n",
      "counter ##################13######################\n",
      "[0.27896792, 0.27896792]\n",
      "counter ##################111######################\n",
      "[nan, nan]\n",
      "counter ##################111######################\n",
      "[nan, nan]\n",
      "counter ##################111######################\n",
      "[nan, nan]\n",
      "counter ##################111######################\n",
      "[nan, nan]\n",
      "counter ##################111######################\n",
      "[nan, nan]\n",
      "counter ##################111######################\n",
      "[nan, nan]\n",
      "counter ##################112######################\n",
      "[1.0000001, 1.0000001]\n",
      "counter ##################112######################\n",
      "[0.9999999, 0.9999999]\n",
      "counter ##################112######################\n",
      "[1.0, 1.0]\n",
      "counter ##################112######################\n",
      "[0.99972993, 0.99972993]\n",
      "counter ##################112######################\n",
      "[1.0, 1.0]\n",
      "counter ##################112######################\n",
      "[0.99719006, 0.99719006]\n",
      "counter ##################113######################\n",
      "[0.9771963, 0.9771963]\n",
      "counter ##################113######################\n",
      "[0.9997274, 0.9997274]\n",
      "counter ##################113######################\n",
      "[0.9500096, 0.9500096]\n",
      "counter ##################113######################\n",
      "[0.99188405, 0.99188405]\n",
      "counter ##################113######################\n",
      "[0.99907297, 0.99907297]\n",
      "counter ##################113######################\n",
      "[0.9815137, 0.9815137]\n",
      "counter ##################114######################\n",
      "[0.969927, 0.969927]\n",
      "counter ##################114######################\n",
      "[0.9832224, 0.9832224]\n",
      "counter ##################114######################\n",
      "[0.8596621, 0.8596621]\n",
      "counter ##################114######################\n",
      "[0.992011, 0.992011]\n",
      "counter ##################114######################\n",
      "[0.99809545, 0.99809545]\n",
      "counter ##################114######################\n",
      "[0.97567374, 0.97567374]\n",
      "counter ##################115######################\n",
      "[0.9706181, 0.9706181]\n",
      "counter ##################115######################\n",
      "[0.986015, 0.986015]\n",
      "counter ##################115######################\n",
      "[0.87923706, 0.87923706]\n",
      "counter ##################115######################\n",
      "[0.9733111, 0.9733111]\n",
      "counter ##################115######################\n",
      "[0.99710774, 0.99710774]\n",
      "counter ##################115######################\n",
      "[0.9447578, 0.9447578]\n",
      "counter ##################116######################\n",
      "[0.9711469, 0.9711469]\n",
      "counter ##################116######################\n",
      "[0.9815407, 0.9815407]\n",
      "counter ##################116######################\n",
      "[0.8815845, 0.8815845]\n",
      "counter ##################116######################\n",
      "[0.97325546, 0.97325546]\n",
      "counter ##################116######################\n",
      "[0.9958605, 0.9958605]\n",
      "counter ##################116######################\n",
      "[0.93008006, 0.93008006]\n",
      "counter ##################117######################\n",
      "[0.971196, 0.971196]\n",
      "counter ##################117######################\n",
      "[0.973543, 0.973543]\n",
      "counter ##################117######################\n",
      "[0.8810337, 0.8810337]\n",
      "counter ##################117######################\n",
      "[0.96293306, 0.96293306]\n",
      "counter ##################117######################\n",
      "[0.9955349, 0.9955349]\n",
      "counter ##################117######################\n",
      "[0.9219559, 0.9219559]\n",
      "counter ##################118######################\n",
      "[0.9700686, 0.9700686]\n",
      "counter ##################118######################\n",
      "[0.96737945, 0.96737945]\n",
      "counter ##################118######################\n",
      "[0.8772802, 0.8772802]\n",
      "counter ##################118######################\n",
      "[0.95278746, 0.95278746]\n",
      "counter ##################118######################\n",
      "[0.99554473, 0.99554473]\n",
      "counter ##################118######################\n",
      "[0.90624416, 0.90624416]\n",
      "counter ##################119######################\n",
      "[0.96887314, 0.96887314]\n",
      "counter ##################119######################\n",
      "[0.935563, 0.935563]\n",
      "counter ##################119######################\n",
      "[0.8879421, 0.8879421]\n",
      "counter ##################119######################\n",
      "[0.93443966, 0.93443966]\n",
      "counter ##################119######################\n",
      "[0.99557984, 0.99557984]\n",
      "counter ##################119######################\n",
      "[0.88867354, 0.88867354]\n",
      "counter ##################120######################\n",
      "[0.9685723, 0.9685723]\n",
      "counter ##################120######################\n",
      "[0.9363251, 0.9363251]\n",
      "counter ##################120######################\n",
      "[0.8945429, 0.8945429]\n",
      "counter ##################120######################\n",
      "[0.9316169, 0.9316169]\n",
      "counter ##################120######################\n",
      "[0.99568444, 0.99568444]\n",
      "counter ##################120######################\n",
      "[0.87924474, 0.87924474]\n",
      "counter ##################121######################\n",
      "[0.96736157, 0.96736157]\n",
      "counter ##################121######################\n",
      "[0.93716997, 0.93716997]\n",
      "counter ##################121######################\n",
      "[0.88468343, 0.88468343]\n",
      "counter ##################121######################\n",
      "[0.87745684, 0.87745684]\n",
      "counter ##################121######################\n",
      "[0.9958588, 0.9958588]\n",
      "counter ##################121######################\n",
      "[0.8872282, 0.8872282]\n",
      "counter ##################122######################\n",
      "[0.966988, 0.966988]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-418-22506159e639>:99: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  return scaled_hsic / (norm_X * norm_Y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter ##################122######################\n",
      "[0.9336242, 0.9336242]\n",
      "counter ##################122######################\n",
      "[0.88437515, 0.88437515]\n",
      "counter ##################122######################\n",
      "[0.86656547, 0.86656547]\n",
      "counter ##################122######################\n",
      "[0.9961022, 0.9961022]\n",
      "counter ##################122######################\n",
      "[0.8762243, 0.8762243]\n",
      "counter ##################123######################\n",
      "[0.96649265, 0.96649265]\n",
      "counter ##################123######################\n",
      "[0.9327534, 0.9327534]\n",
      "counter ##################123######################\n",
      "[0.873816, 0.873816]\n",
      "counter ##################123######################\n",
      "[0.86913764, 0.86913764]\n",
      "counter ##################123######################\n",
      "[0.9963832, 0.9963832]\n",
      "counter ##################123######################\n",
      "[0.8724625, 0.8724625]\n",
      "counter ##################124######################\n",
      "[0.9655038, 0.9655038]\n",
      "counter ##################124######################\n",
      "[0.93306696, 0.93306696]\n",
      "counter ##################124######################\n",
      "[0.88043743, 0.88043743]\n",
      "counter ##################124######################\n",
      "[0.8432095, 0.8432095]\n",
      "counter ##################124######################\n",
      "[0.9966947, 0.9966947]\n",
      "counter ##################124######################\n",
      "[0.86093783, 0.86093783]\n",
      "counter ##################125######################\n",
      "[0.9651047, 0.9651047]\n",
      "counter ##################125######################\n",
      "[0.9330046, 0.9330046]\n",
      "counter ##################125######################\n",
      "[0.8838963, 0.8838963]\n",
      "counter ##################125######################\n",
      "[0.79146487, 0.79146487]\n",
      "counter ##################125######################\n",
      "[0.996951, 0.996951]\n",
      "counter ##################125######################\n",
      "[0.87337106, 0.87337106]\n",
      "counter ##################126######################\n",
      "[0.964892, 0.964892]\n",
      "counter ##################126######################\n",
      "[0.93434906, 0.93434906]\n",
      "counter ##################126######################\n",
      "[0.8868921, 0.8868921]\n",
      "counter ##################126######################\n",
      "[0.77902365, 0.77902365]\n",
      "counter ##################126######################\n",
      "[0.99715483, 0.99715483]\n",
      "counter ##################126######################\n",
      "[0.866789, 0.866789]\n",
      "counter ##################127######################\n",
      "[0.9644531, 0.9644531]\n",
      "counter ##################127######################\n",
      "[0.93352777, 0.93352777]\n",
      "counter ##################127######################\n",
      "[0.88975304, 0.88975304]\n",
      "counter ##################127######################\n",
      "[0.7843357, 0.7843357]\n",
      "counter ##################127######################\n",
      "[0.9973407, 0.9973407]\n",
      "counter ##################127######################\n",
      "[0.8462919, 0.8462919]\n",
      "counter ##################128######################\n",
      "[0.9637694, 0.9637694]\n",
      "counter ##################128######################\n",
      "[0.9304755, 0.9304755]\n",
      "counter ##################128######################\n",
      "[0.8876954, 0.8876954]\n",
      "counter ##################128######################\n",
      "[0.79806775, 0.79806775]\n",
      "counter ##################128######################\n",
      "[0.99751526, 0.99751526]\n",
      "counter ##################128######################\n",
      "[0.8711338, 0.8711338]\n",
      "counter ##################129######################\n",
      "[0.96314245, 0.96314245]\n",
      "counter ##################129######################\n",
      "[0.9158292, 0.9158292]\n",
      "counter ##################129######################\n",
      "[0.8822842, 0.8822842]\n",
      "counter ##################129######################\n",
      "[0.7962218, 0.7962218]\n",
      "counter ##################129######################\n",
      "[0.9976496, 0.9976496]\n",
      "counter ##################129######################\n",
      "[0.86913526, 0.86913526]\n",
      "counter ##################130######################\n",
      "[0.96282315, 0.96282315]\n",
      "counter ##################130######################\n",
      "[0.91250366, 0.91250366]\n",
      "counter ##################130######################\n",
      "[0.8872015, 0.8872015]\n",
      "counter ##################130######################\n",
      "[0.80697894, 0.80697894]\n",
      "counter ##################130######################\n",
      "[0.99775887, 0.99775887]\n",
      "counter ##################130######################\n",
      "[0.8331749, 0.8331749]\n",
      "counter ##################131######################\n",
      "[0.9625448, 0.9625448]\n",
      "counter ##################131######################\n",
      "[0.90891993, 0.90891993]\n",
      "counter ##################131######################\n",
      "[0.8782016, 0.8782016]\n",
      "counter ##################131######################\n",
      "[0.7988184, 0.7988184]\n",
      "counter ##################131######################\n",
      "[0.99785155, 0.99785155]\n",
      "counter ##################131######################\n",
      "[0.8066792, 0.8066792]\n",
      "counter ##################132######################\n",
      "[0.96211934, 0.96211934]\n",
      "counter ##################132######################\n",
      "[0.88860685, 0.88860685]\n",
      "counter ##################132######################\n",
      "[0.87765175, 0.87765175]\n",
      "counter ##################132######################\n",
      "[0.79709506, 0.79709506]\n",
      "counter ##################132######################\n",
      "[0.99796903, 0.99796903]\n",
      "counter ##################132######################\n",
      "[0.8592354, 0.8592354]\n",
      "counter ##################133######################\n",
      "[0.96168023, 0.96168023]\n",
      "counter ##################133######################\n",
      "[0.89069706, 0.89069706]\n",
      "counter ##################133######################\n",
      "[0.87831676, 0.87831676]\n",
      "counter ##################133######################\n",
      "[0.7881677, 0.7881677]\n",
      "counter ##################133######################\n",
      "[0.9980729, 0.9980729]\n",
      "counter ##################133######################\n",
      "[0.8446483, 0.8446483]\n",
      "counter ##################134######################\n",
      "[0.96115094, 0.96115094]\n",
      "counter ##################134######################\n",
      "[0.8917693, 0.8917693]\n",
      "counter ##################134######################\n",
      "[0.8747089, 0.8747089]\n",
      "counter ##################134######################\n",
      "[0.7848356, 0.7848356]\n",
      "counter ##################134######################\n",
      "[0.9981623, 0.9981623]\n",
      "counter ##################134######################\n",
      "[0.856187, 0.856187]\n",
      "counter ##################135######################\n",
      "[0.96079195, 0.96079195]\n",
      "counter ##################135######################\n",
      "[0.89038616, 0.89038616]\n",
      "counter ##################135######################\n",
      "[0.8786828, 0.8786828]\n",
      "counter ##################135######################\n",
      "[0.7816661, 0.7816661]\n",
      "counter ##################135######################\n",
      "[0.99825495, 0.99825495]\n",
      "counter ##################135######################\n",
      "[0.84646064, 0.84646064]\n",
      "counter ##################136######################\n",
      "[0.9602989, 0.9602989]\n",
      "counter ##################136######################\n",
      "[0.88963795, 0.88963795]\n",
      "counter ##################136######################\n",
      "[0.87920034, 0.87920034]\n",
      "counter ##################136######################\n",
      "[0.78398836, 0.78398836]\n",
      "counter ##################136######################\n",
      "[0.9983318, 0.9983318]\n",
      "counter ##################136######################\n",
      "[0.8419795, 0.8419795]\n",
      "counter ##################137######################\n",
      "[0.9598006, 0.9598006]\n",
      "counter ##################137######################\n",
      "[0.8907883, 0.8907883]\n",
      "counter ##################137######################\n",
      "[0.8793099, 0.8793099]\n",
      "counter ##################137######################\n",
      "[0.78641397, 0.78641397]\n",
      "counter ##################137######################\n",
      "[0.9984135, 0.9984135]\n",
      "counter ##################137######################\n",
      "[0.85902387, 0.85902387]\n",
      "counter ##################138######################\n",
      "[0.9592912, 0.9592912]\n",
      "counter ##################138######################\n",
      "[0.89019114, 0.89019114]\n",
      "counter ##################138######################\n",
      "[0.8799414, 0.8799414]\n",
      "counter ##################138######################\n",
      "[0.7820599, 0.7820599]\n",
      "counter ##################138######################\n",
      "[0.9984852, 0.9984852]\n",
      "counter ##################138######################\n",
      "[0.8488483, 0.8488483]\n",
      "counter ##################139######################\n",
      "[0.95880264, 0.95880264]\n",
      "counter ##################139######################\n",
      "[0.8873325, 0.8873325]\n",
      "counter ##################139######################\n",
      "[0.88323283, 0.88323283]\n",
      "counter ##################139######################\n",
      "[0.7835206, 0.7835206]\n",
      "counter ##################139######################\n",
      "[0.9985554, 0.9985554]\n",
      "counter ##################139######################\n",
      "[0.81704324, 0.81704324]\n",
      "counter ##################140######################\n",
      "[0.9583696, 0.9583696]\n",
      "counter ##################140######################\n",
      "[0.8874556, 0.8874556]\n",
      "counter ##################140######################\n",
      "[0.88298625, 0.88298625]\n",
      "counter ##################140######################\n",
      "[0.7848315, 0.7848315]\n",
      "counter ##################140######################\n",
      "[0.9986181, 0.9986181]\n",
      "counter ##################140######################\n",
      "[0.8553728, 0.8553728]\n",
      "counter ##################141######################\n",
      "[0.9579749, 0.9579749]\n",
      "counter ##################141######################\n",
      "[0.8862825, 0.8862825]\n",
      "counter ##################141######################\n",
      "[0.88360316, 0.88360316]\n",
      "counter ##################141######################\n",
      "[0.77880794, 0.77880794]\n",
      "counter ##################141######################\n",
      "[0.9986735, 0.9986735]\n",
      "counter ##################141######################\n",
      "[0.8420068, 0.8420068]\n",
      "counter ##################142######################\n",
      "[0.9576043, 0.9576043]\n",
      "counter ##################142######################\n",
      "[0.8856831, 0.8856831]\n",
      "counter ##################142######################\n",
      "[0.8835218, 0.8835218]\n",
      "counter ##################142######################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7788555, 0.7788555]\n",
      "counter ##################142######################\n",
      "[0.9987287, 0.9987287]\n",
      "counter ##################142######################\n",
      "[0.8418409, 0.8418409]\n",
      "counter ##################143######################\n",
      "[0.95717084, 0.95717084]\n",
      "counter ##################143######################\n",
      "[0.8851536, 0.8851536]\n",
      "counter ##################143######################\n",
      "[0.885698, 0.885698]\n",
      "counter ##################143######################\n",
      "[0.7816153, 0.7816153]\n",
      "counter ##################143######################\n",
      "[0.99877846, 0.99877846]\n",
      "counter ##################143######################\n",
      "[0.8483503, 0.8483503]\n",
      "counter ##################144######################\n",
      "[0.9567385, 0.9567385]\n",
      "counter ##################144######################\n",
      "[0.86458343, 0.86458343]\n",
      "counter ##################144######################\n",
      "[0.88747835, 0.88747835]\n",
      "counter ##################144######################\n",
      "[0.7825431, 0.7825431]\n",
      "counter ##################144######################\n",
      "[0.99882406, 0.99882406]\n",
      "counter ##################144######################\n",
      "[0.83379877, 0.83379877]\n",
      "counter ##################145######################\n",
      "[0.95629376, 0.95629376]\n",
      "counter ##################145######################\n",
      "[0.85999036, 0.85999036]\n",
      "counter ##################145######################\n",
      "[0.889952, 0.889952]\n",
      "counter ##################145######################\n",
      "[0.7826392, 0.7826392]\n",
      "counter ##################145######################\n",
      "[0.9988686, 0.9988686]\n",
      "counter ##################145######################\n",
      "[0.81111956, 0.81111956]\n",
      "counter ##################146######################\n",
      "[0.9559967, 0.9559967]\n",
      "counter ##################146######################\n",
      "[0.8590224, 0.8590224]\n",
      "counter ##################146######################\n",
      "[0.88999844, 0.88999844]\n",
      "counter ##################146######################\n",
      "[0.7824212, 0.7824212]\n",
      "counter ##################146######################\n",
      "[0.9989097, 0.9989097]\n",
      "counter ##################146######################\n",
      "[0.82074934, 0.82074934]\n",
      "counter ##################147######################\n",
      "[0.95573497, 0.95573497]\n",
      "counter ##################147######################\n",
      "[0.85606253, 0.85606253]\n",
      "counter ##################147######################\n",
      "[0.8909352, 0.8909352]\n",
      "counter ##################147######################\n",
      "[0.7883475, 0.7883475]\n",
      "counter ##################147######################\n",
      "[0.99894845, 0.99894845]\n",
      "counter ##################147######################\n",
      "[0.81997776, 0.81997776]\n",
      "counter ##################148######################\n",
      "[0.9553225, 0.9553225]\n",
      "counter ##################148######################\n",
      "[0.85459423, 0.85459423]\n",
      "counter ##################148######################\n",
      "[0.8928048, 0.8928048]\n",
      "counter ##################148######################\n",
      "[0.7849328, 0.7849328]\n",
      "counter ##################148######################\n",
      "[0.9989828, 0.9989828]\n",
      "counter ##################148######################\n",
      "[0.8265892, 0.8265892]\n",
      "counter ##################149######################\n",
      "[0.95496154, 0.95496154]\n",
      "counter ##################149######################\n",
      "[0.8543396, 0.8543396]\n",
      "counter ##################149######################\n",
      "[0.89313096, 0.89313096]\n",
      "counter ##################149######################\n",
      "[0.7830765, 0.7830765]\n",
      "counter ##################149######################\n",
      "[0.99901587, 0.99901587]\n",
      "counter ##################149######################\n",
      "[0.82546985, 0.82546985]\n",
      "counter ##################150######################\n",
      "[0.95465684, 0.95465684]\n",
      "counter ##################150######################\n",
      "[0.8460047, 0.8460047]\n",
      "counter ##################150######################\n",
      "[0.89519334, 0.89519334]\n",
      "counter ##################150######################\n",
      "[0.78269523, 0.78269523]\n",
      "counter ##################150######################\n",
      "[0.9990461, 0.9990461]\n",
      "counter ##################150######################\n",
      "[0.82919705, 0.82919705]\n",
      "counter ##################151######################\n",
      "[0.9543952, 0.9543952]\n",
      "counter ##################151######################\n",
      "[0.83471435, 0.83471435]\n",
      "counter ##################151######################\n",
      "[0.8945504, 0.8945504]\n",
      "counter ##################151######################\n",
      "[0.7857114, 0.7857114]\n",
      "counter ##################151######################\n",
      "[0.99907196, 0.99907196]\n",
      "counter ##################151######################\n",
      "[0.8347201, 0.8347201]\n",
      "counter ##################152######################\n",
      "[0.9541619, 0.9541619]\n",
      "counter ##################152######################\n",
      "[0.8266338, 0.8266338]\n",
      "counter ##################152######################\n",
      "[0.89451593, 0.89451593]\n",
      "counter ##################152######################\n",
      "[0.78921366, 0.78921366]\n",
      "counter ##################152######################\n",
      "[0.9990955, 0.9990955]\n",
      "counter ##################152######################\n",
      "[0.7889788, 0.7889788]\n",
      "counter ##################153######################\n",
      "[0.9539312, 0.9539312]\n",
      "counter ##################153######################\n",
      "[0.8261311, 0.8261311]\n",
      "counter ##################153######################\n",
      "[0.8951461, 0.8951461]\n",
      "counter ##################153######################\n",
      "[0.79158115, 0.79158115]\n",
      "counter ##################153######################\n",
      "[0.9991183, 0.9991183]\n",
      "counter ##################153######################\n",
      "[0.8275875, 0.8275875]\n",
      "counter ##################154######################\n",
      "[0.9536831, 0.9536831]\n",
      "counter ##################154######################\n",
      "[0.81615955, 0.81615955]\n",
      "counter ##################154######################\n",
      "[0.89481544, 0.89481544]\n",
      "counter ##################154######################\n",
      "[0.79963166, 0.79963166]\n",
      "counter ##################154######################\n",
      "[0.9991401, 0.9991401]\n",
      "counter ##################154######################\n",
      "[0.8294711, 0.8294711]\n",
      "counter ##################155######################\n",
      "[0.9534302, 0.9534302]\n",
      "counter ##################155######################\n",
      "[0.8166976, 0.8166976]\n",
      "counter ##################155######################\n",
      "[0.89506274, 0.89506274]\n",
      "counter ##################155######################\n",
      "[0.79721826, 0.79721826]\n",
      "counter ##################155######################\n",
      "[0.9991606, 0.9991606]\n",
      "counter ##################155######################\n",
      "[0.81363785, 0.81363785]\n",
      "counter ##################156######################\n",
      "[0.9532269, 0.9532269]\n",
      "counter ##################156######################\n",
      "[0.8120257, 0.8120257]\n",
      "counter ##################156######################\n",
      "[0.89592326, 0.89592326]\n",
      "counter ##################156######################\n",
      "[0.79296976, 0.79296976]\n",
      "counter ##################156######################\n",
      "[0.9991819, 0.9991819]\n",
      "counter ##################156######################\n",
      "[0.82420003, 0.82420003]\n",
      "counter ##################157######################\n",
      "[0.9529429, 0.9529429]\n",
      "counter ##################157######################\n",
      "[0.81191224, 0.81191224]\n",
      "counter ##################157######################\n",
      "[0.89507884, 0.89507884]\n",
      "counter ##################157######################\n",
      "[0.7942803, 0.7942803]\n",
      "counter ##################157######################\n",
      "[0.99920124, 0.99920124]\n",
      "counter ##################157######################\n",
      "[0.8293495, 0.8293495]\n",
      "counter ##################158######################\n",
      "[0.9527947, 0.9527947]\n",
      "counter ##################158######################\n",
      "[0.80938697, 0.80938697]\n",
      "counter ##################158######################\n",
      "[0.89508593, 0.89508593]\n",
      "counter ##################158######################\n",
      "[0.79812324, 0.79812324]\n",
      "counter ##################158######################\n",
      "[0.99921924, 0.99921924]\n",
      "counter ##################158######################\n",
      "[0.798842, 0.798842]\n",
      "counter ##################159######################\n",
      "[0.9526501, 0.9526501]\n",
      "counter ##################159######################\n",
      "[0.8005485, 0.8005485]\n",
      "counter ##################159######################\n",
      "[0.8945069, 0.8945069]\n",
      "counter ##################159######################\n",
      "[0.7987432, 0.7987432]\n",
      "counter ##################159######################\n",
      "[0.99923515, 0.99923515]\n",
      "counter ##################159######################\n",
      "[0.83978325, 0.83978325]\n",
      "counter ##################160######################\n",
      "[0.952529, 0.952529]\n",
      "counter ##################160######################\n",
      "[0.7933146, 0.7933146]\n",
      "counter ##################160######################\n",
      "[0.8948397, 0.8948397]\n",
      "counter ##################160######################\n",
      "[0.80258334, 0.80258334]\n",
      "counter ##################160######################\n",
      "[0.99924785, 0.99924785]\n",
      "counter ##################160######################\n",
      "[0.8488759, 0.8488759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "id": "os9tTnaLmGvV",
    "outputId": "d2eaaee3-32bf-40a9-f85b-9016fc0daea6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pb3Z8U0UmGvV"
   },
   "source": [
    "## BLEU\n",
    "\n",
    "Finally we calculate the BLEU score for the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "id": "-V2Fq33ymGvV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    speeder = 0    \n",
    "    for datum in data:\n",
    "        speeder += 1\n",
    "        if speeder == 3:\n",
    "            return bleu_score(pred_trgs, trgs)\n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhT-u0F0mGvV"
   },
   "source": [
    "We get a BLEU score of 36.52, which beats the ~34 of the convolutional sequence-to-sequence model and ~28 of the attention based RNN model. All this whilst having the least amount of parameters and the fastest training time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "id": "jsqe4c2BmGvV",
    "outputId": "f33f78be-dff4-456c-e4ad-060595f516b5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter ##################14######################\n",
      "[0.38249674, 0.38249674]\n",
      "counter ##################14######################\n",
      "[0.28098065, 0.28098065]\n",
      "counter ##################14######################\n",
      "[0.47255605, 0.47255605]\n",
      "counter ##################161######################\n",
      "[nan, nan]\n",
      "counter ##################161######################\n",
      "[nan, nan]\n",
      "counter ##################161######################\n",
      "[nan, nan]\n",
      "counter ##################161######################\n",
      "[nan, nan]\n",
      "counter ##################161######################\n",
      "[nan, nan]\n",
      "counter ##################161######################\n",
      "[nan, nan]\n",
      "counter ##################162######################\n",
      "[1.0000001, 1.0000001]\n",
      "counter ##################162######################\n",
      "[0.9999994, 0.9999994]\n",
      "counter ##################162######################\n",
      "[1.0, 1.0]\n",
      "counter ##################162######################\n",
      "[0.99979424, 0.99979424]\n",
      "counter ##################162######################\n",
      "[1.0, 1.0]\n",
      "counter ##################162######################\n",
      "[0.95292574, 0.95292574]\n",
      "counter ##################163######################\n",
      "[0.9771963, 0.9771963]\n",
      "counter ##################163######################\n",
      "[0.9518717, 0.9518717]\n",
      "counter ##################163######################\n",
      "[0.9517949, 0.9517949]\n",
      "counter ##################163######################\n",
      "[0.9963878, 0.9963878]\n",
      "counter ##################163######################\n",
      "[0.9992164, 0.9992164]\n",
      "counter ##################163######################\n",
      "[0.95112497, 0.95112497]\n",
      "counter ##################164######################\n",
      "[0.969927, 0.969927]\n",
      "counter ##################164######################\n",
      "[0.94654936, 0.94654936]\n",
      "counter ##################164######################\n",
      "[0.8781358, 0.8781358]\n",
      "counter ##################164######################\n",
      "[0.9877105, 0.9877105]\n",
      "counter ##################164######################\n",
      "[0.9982799, 0.9982799]\n",
      "counter ##################164######################\n",
      "[0.9177797, 0.9177797]\n",
      "counter ##################165######################\n",
      "[0.9706181, 0.9706181]\n",
      "counter ##################165######################\n",
      "[0.9499977, 0.9499977]\n",
      "counter ##################165######################\n",
      "[0.89841384, 0.89841384]\n",
      "counter ##################165######################\n",
      "[0.9829262, 0.9829262]\n",
      "counter ##################165######################\n",
      "[0.9971839, 0.9971839]\n",
      "counter ##################165######################\n",
      "[0.9442526, 0.9442526]\n",
      "counter ##################166######################\n",
      "[0.9711469, 0.9711469]\n",
      "counter ##################166######################\n",
      "[0.94545466, 0.94545466]\n",
      "counter ##################166######################\n",
      "[0.9031821, 0.9031821]\n",
      "counter ##################166######################\n",
      "[0.97715515, 0.97715515]\n",
      "counter ##################166######################\n",
      "[0.99575466, 0.99575466]\n",
      "counter ##################166######################\n",
      "[0.93165183, 0.93165183]\n",
      "counter ##################167######################\n",
      "[0.971196, 0.971196]\n",
      "counter ##################167######################\n",
      "[0.9425624, 0.9425624]\n",
      "counter ##################167######################\n",
      "[0.9029232, 0.9029232]\n",
      "counter ##################167######################\n",
      "[0.9759685, 0.9759685]\n",
      "counter ##################167######################\n",
      "[0.99526787, 0.99526787]\n",
      "counter ##################167######################\n",
      "[0.9105747, 0.9105747]\n",
      "counter ##################168######################\n",
      "[0.9700686, 0.9700686]\n",
      "counter ##################168######################\n",
      "[0.93510026, 0.93510026]\n",
      "counter ##################168######################\n",
      "[0.8966031, 0.8966031]\n",
      "counter ##################168######################\n",
      "[0.9554662, 0.9554662]\n",
      "counter ##################168######################\n",
      "[0.9951786, 0.9951786]\n",
      "counter ##################168######################\n",
      "[0.91138655, 0.91138655]\n",
      "counter ##################169######################\n",
      "[0.96887314, 0.96887314]\n",
      "counter ##################169######################\n",
      "[0.91464823, 0.91464823]\n",
      "counter ##################169######################\n",
      "[0.9063642, 0.9063642]\n",
      "counter ##################169######################\n",
      "[0.9383943, 0.9383943]\n",
      "counter ##################169######################\n",
      "[0.99513626, 0.99513626]\n",
      "counter ##################169######################\n",
      "[0.9065368, 0.9065368]\n",
      "counter ##################170######################\n",
      "[0.9685723, 0.9685723]\n",
      "counter ##################170######################\n",
      "[0.9121927, 0.9121927]\n",
      "counter ##################170######################\n",
      "[0.9127619, 0.9127619]\n",
      "counter ##################170######################\n",
      "[0.9375649, 0.9375649]\n",
      "counter ##################170######################\n",
      "[0.99520004, 0.99520004]\n",
      "counter ##################170######################\n",
      "[0.9040493, 0.9040493]\n",
      "counter ##################171######################\n",
      "[0.96736157, 0.96736157]\n",
      "counter ##################171######################\n",
      "[0.9077867, 0.9077867]\n",
      "counter ##################171######################\n",
      "[0.90367687, 0.90367687]\n",
      "counter ##################171######################\n",
      "[0.8921683, 0.8921683]\n",
      "counter ##################171######################\n",
      "[0.9953268, 0.9953268]\n",
      "counter ##################171######################\n",
      "[0.8784482, 0.8784482]\n",
      "counter ##################172######################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-418-22506159e639>:99: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  return scaled_hsic / (norm_X * norm_Y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.966988, 0.966988]\n",
      "counter ##################172######################\n",
      "[0.8984508, 0.8984508]\n",
      "counter ##################172######################\n",
      "[0.90261436, 0.90261436]\n",
      "counter ##################172######################\n",
      "[0.85820365, 0.85820365]\n",
      "counter ##################172######################\n",
      "[0.9955547, 0.9955547]\n",
      "counter ##################172######################\n",
      "[0.87354046, 0.87354046]\n",
      "counter ##################173######################\n",
      "[0.96649265, 0.96649265]\n",
      "counter ##################173######################\n",
      "[0.89883995, 0.89883995]\n",
      "counter ##################173######################\n",
      "[0.89285064, 0.89285064]\n",
      "counter ##################173######################\n",
      "[0.82333577, 0.82333577]\n",
      "counter ##################173######################\n",
      "[0.9958267, 0.9958267]\n",
      "counter ##################173######################\n",
      "[0.90727615, 0.90727615]\n",
      "counter ##################174######################\n",
      "[0.9655038, 0.9655038]\n",
      "counter ##################174######################\n",
      "[0.90018386, 0.90018386]\n",
      "counter ##################174######################\n",
      "[0.89829236, 0.89829236]\n",
      "counter ##################174######################\n",
      "[0.78194165, 0.78194165]\n",
      "counter ##################174######################\n",
      "[0.99615294, 0.99615294]\n",
      "counter ##################174######################\n",
      "[0.82987446, 0.82987446]\n",
      "counter ##################175######################\n",
      "[0.9651047, 0.9651047]\n",
      "counter ##################175######################\n",
      "[0.8970497, 0.8970497]\n",
      "counter ##################175######################\n",
      "[0.9019111, 0.9019111]\n",
      "counter ##################175######################\n",
      "[0.76390946, 0.76390946]\n",
      "counter ##################175######################\n",
      "[0.9964257, 0.9964257]\n",
      "counter ##################175######################\n",
      "[0.8390093, 0.8390093]\n",
      "counter ##################176######################\n",
      "[0.964892, 0.964892]\n",
      "counter ##################176######################\n",
      "[0.8971477, 0.8971477]\n",
      "counter ##################176######################\n",
      "[0.9051193, 0.9051193]\n",
      "counter ##################176######################\n",
      "[0.73693556, 0.73693556]\n",
      "counter ##################176######################\n",
      "[0.99664867, 0.99664867]\n",
      "counter ##################176######################\n",
      "[0.84161407, 0.84161407]\n",
      "counter ##################177######################\n",
      "[0.9644531, 0.9644531]\n",
      "counter ##################177######################\n",
      "[0.8900785, 0.8900785]\n",
      "counter ##################177######################\n",
      "[0.90732425, 0.90732425]\n",
      "counter ##################177######################\n",
      "[0.73380584, 0.73380584]\n",
      "counter ##################177######################\n",
      "[0.9968509, 0.9968509]\n",
      "counter ##################177######################\n",
      "[0.85501957, 0.85501957]\n",
      "counter ##################178######################\n",
      "[0.9637694, 0.9637694]\n",
      "counter ##################178######################\n",
      "[0.8888209, 0.8888209]\n",
      "counter ##################178######################\n",
      "[0.9039346, 0.9039346]\n",
      "counter ##################178######################\n",
      "[0.74453646, 0.74453646]\n",
      "counter ##################178######################\n",
      "[0.99704164, 0.99704164]\n",
      "counter ##################178######################\n",
      "[0.8524422, 0.8524422]\n",
      "counter ##################179######################\n",
      "[0.96314245, 0.96314245]\n",
      "counter ##################179######################\n",
      "[0.89166886, 0.89166886]\n",
      "counter ##################179######################\n",
      "[0.8995664, 0.8995664]\n",
      "counter ##################179######################\n",
      "[0.74607813, 0.74607813]\n",
      "counter ##################179######################\n",
      "[0.99718845, 0.99718845]\n",
      "counter ##################179######################\n",
      "[0.8362918, 0.8362918]\n",
      "counter ##################180######################\n",
      "[0.96282315, 0.96282315]\n",
      "counter ##################180######################\n",
      "[0.88559705, 0.88559705]\n",
      "counter ##################180######################\n",
      "[0.9041216, 0.9041216]\n",
      "counter ##################180######################\n",
      "[0.7519054, 0.7519054]\n",
      "counter ##################180######################\n",
      "[0.9973083, 0.9973083]\n",
      "counter ##################180######################\n",
      "[0.82827467, 0.82827467]\n",
      "counter ##################181######################\n",
      "[0.9625448, 0.9625448]\n",
      "counter ##################181######################\n",
      "[0.88381034, 0.88381034]\n",
      "counter ##################181######################\n",
      "[0.8955119, 0.8955119]\n",
      "counter ##################181######################\n",
      "[0.7471197, 0.7471197]\n",
      "counter ##################181######################\n",
      "[0.9974077, 0.9974077]\n",
      "counter ##################181######################\n",
      "[0.81088465, 0.81088465]\n",
      "counter ##################182######################\n",
      "[0.96211934, 0.96211934]\n",
      "counter ##################182######################\n",
      "[0.8837983, 0.8837983]\n",
      "counter ##################182######################\n",
      "[0.8945331, 0.8945331]\n",
      "counter ##################182######################\n",
      "[0.7404659, 0.7404659]\n",
      "counter ##################182######################\n",
      "[0.9975374, 0.9975374]\n",
      "counter ##################182######################\n",
      "[0.8142815, 0.8142815]\n",
      "counter ##################183######################\n",
      "[0.96168023, 0.96168023]\n",
      "counter ##################183######################\n",
      "[0.8843899, 0.8843899]\n",
      "counter ##################183######################\n",
      "[0.8952438, 0.8952438]\n",
      "counter ##################183######################\n",
      "[0.7460974, 0.7460974]\n",
      "counter ##################183######################\n",
      "[0.9976492, 0.9976492]\n",
      "counter ##################183######################\n",
      "[0.73324406, 0.73324406]\n",
      "counter ##################184######################\n",
      "[0.96115094, 0.96115094]\n",
      "counter ##################184######################\n",
      "[0.8777669, 0.8777669]\n",
      "counter ##################184######################\n",
      "[0.88995785, 0.88995785]\n",
      "counter ##################184######################\n",
      "[0.75575984, 0.75575984]\n",
      "counter ##################184######################\n",
      "[0.9977464, 0.9977464]\n",
      "counter ##################184######################\n",
      "[0.7796048, 0.7796048]\n",
      "counter ##################185######################\n",
      "[0.96079195, 0.96079195]\n",
      "counter ##################185######################\n",
      "[0.8713318, 0.8713318]\n",
      "counter ##################185######################\n",
      "[0.8937421, 0.8937421]\n",
      "counter ##################185######################\n",
      "[0.7368068, 0.7368068]\n",
      "counter ##################185######################\n",
      "[0.99784935, 0.99784935]\n",
      "counter ##################185######################\n",
      "[0.76198584, 0.76198584]\n",
      "counter ##################186######################\n",
      "[0.9602989, 0.9602989]\n",
      "counter ##################186######################\n",
      "[0.8663915, 0.8663915]\n",
      "counter ##################186######################\n",
      "[0.89445454, 0.89445454]\n",
      "counter ##################186######################\n",
      "[0.74560535, 0.74560535]\n",
      "counter ##################186######################\n",
      "[0.9979352, 0.9979352]\n",
      "counter ##################186######################\n",
      "[0.78954256, 0.78954256]\n",
      "counter ##################187######################\n",
      "[0.9598006, 0.9598006]\n",
      "counter ##################187######################\n",
      "[0.8640709, 0.8640709]\n",
      "counter ##################187######################\n",
      "[0.8951709, 0.8951709]\n",
      "counter ##################187######################\n",
      "[0.7542721, 0.7542721]\n",
      "counter ##################187######################\n",
      "[0.99802715, 0.99802715]\n",
      "counter ##################187######################\n",
      "[0.7390775, 0.7390775]\n",
      "counter ##################188######################\n",
      "[0.9592912, 0.9592912]\n",
      "counter ##################188######################\n",
      "[0.8601024, 0.8601024]\n",
      "counter ##################188######################\n",
      "[0.8959783, 0.8959783]\n",
      "counter ##################188######################\n",
      "[0.75353616, 0.75353616]\n",
      "counter ##################188######################\n",
      "[0.99810904, 0.99810904]\n",
      "counter ##################188######################\n",
      "[0.71817887, 0.71817887]\n",
      "counter ##################189######################\n",
      "[0.95880264, 0.95880264]\n",
      "counter ##################189######################\n",
      "[0.86021584, 0.86021584]\n",
      "counter ##################189######################\n",
      "[0.8990343, 0.8990343]\n",
      "counter ##################189######################\n",
      "[0.75254285, 0.75254285]\n",
      "counter ##################189######################\n",
      "[0.99819, 0.99819]\n",
      "counter ##################189######################\n",
      "[0.75481904, 0.75481904]\n",
      "counter ##################190######################\n",
      "[0.9583696, 0.9583696]\n",
      "counter ##################190######################\n",
      "[0.85979587, 0.85979587]\n",
      "counter ##################190######################\n",
      "[0.89838845, 0.89838845]\n",
      "counter ##################190######################\n",
      "[0.75341016, 0.75341016]\n",
      "counter ##################190######################\n",
      "[0.99826026, 0.99826026]\n",
      "counter ##################190######################\n",
      "[0.7724218, 0.7724218]\n",
      "counter ##################191######################\n",
      "[0.9579749, 0.9579749]\n",
      "counter ##################191######################\n",
      "[0.85532516, 0.85532516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter ##################191######################\n",
      "[0.8986938, 0.8986938]\n",
      "counter ##################191######################\n",
      "[0.75811803, 0.75811803]\n",
      "counter ##################191######################\n",
      "[0.99832, 0.99832]\n",
      "counter ##################191######################\n",
      "[0.7596318, 0.7596318]\n",
      "counter ##################192######################\n",
      "[0.9576043, 0.9576043]\n",
      "counter ##################192######################\n",
      "[0.8522617, 0.8522617]\n",
      "counter ##################192######################\n",
      "[0.897895, 0.897895]\n",
      "counter ##################192######################\n",
      "[0.757437, 0.757437]\n",
      "counter ##################192######################\n",
      "[0.99838245, 0.99838245]\n",
      "counter ##################192######################\n",
      "[0.7224163, 0.7224163]\n",
      "counter ##################193######################\n",
      "[0.95717084, 0.95717084]\n",
      "counter ##################193######################\n",
      "[0.8460262, 0.8460262]\n",
      "counter ##################193######################\n",
      "[0.89994985, 0.89994985]\n",
      "counter ##################193######################\n",
      "[0.7516841, 0.7516841]\n",
      "counter ##################193######################\n",
      "[0.9984384, 0.9984384]\n",
      "counter ##################193######################\n",
      "[0.7300117, 0.7300117]\n",
      "counter ##################194######################\n",
      "[0.9567385, 0.9567385]\n",
      "counter ##################194######################\n",
      "[0.8115285, 0.8115285]\n",
      "counter ##################194######################\n",
      "[0.9014467, 0.9014467]\n",
      "counter ##################194######################\n",
      "[0.7530207, 0.7530207]\n",
      "counter ##################194######################\n",
      "[0.99849117, 0.99849117]\n",
      "counter ##################194######################\n",
      "[0.69317263, 0.69317263]\n",
      "counter ##################195######################\n",
      "[0.95629376, 0.95629376]\n",
      "counter ##################195######################\n",
      "[0.8065163, 0.8065163]\n",
      "counter ##################195######################\n",
      "[0.90365046, 0.90365046]\n",
      "counter ##################195######################\n",
      "[0.7550953, 0.7550953]\n",
      "counter ##################195######################\n",
      "[0.9985444, 0.9985444]\n",
      "counter ##################195######################\n",
      "[0.704312, 0.704312]\n",
      "counter ##################196######################\n",
      "[0.9559967, 0.9559967]\n",
      "counter ##################196######################\n",
      "[0.80478954, 0.80478954]\n",
      "counter ##################196######################\n",
      "[0.9038189, 0.9038189]\n",
      "counter ##################196######################\n",
      "[0.7489348, 0.7489348]\n",
      "counter ##################196######################\n",
      "[0.99859506, 0.99859506]\n",
      "counter ##################196######################\n",
      "[0.64551467, 0.64551467]\n",
      "counter ##################197######################\n",
      "[0.95573497, 0.95573497]\n",
      "counter ##################197######################\n",
      "[0.7913811, 0.7913811]\n",
      "counter ##################197######################\n",
      "[0.90472996, 0.90472996]\n",
      "counter ##################197######################\n",
      "[0.7498934, 0.7498934]\n",
      "counter ##################197######################\n",
      "[0.9986438, 0.9986438]\n",
      "counter ##################197######################\n",
      "[0.68607754, 0.68607754]\n",
      "counter ##################198######################\n",
      "[0.9553225, 0.9553225]\n",
      "counter ##################198######################\n",
      "[0.7885579, 0.7885579]\n",
      "counter ##################198######################\n",
      "[0.9065375, 0.9065375]\n",
      "counter ##################198######################\n",
      "[0.7485042, 0.7485042]\n",
      "counter ##################198######################\n",
      "[0.9986891, 0.9986891]\n",
      "counter ##################198######################\n",
      "[0.6963576, 0.6963576]\n",
      "counter ##################199######################\n",
      "[0.95496154, 0.95496154]\n",
      "counter ##################199######################\n",
      "[0.77694327, 0.77694327]\n",
      "counter ##################199######################\n",
      "[0.90688235, 0.90688235]\n",
      "counter ##################199######################\n",
      "[0.7574927, 0.7574927]\n",
      "counter ##################199######################\n",
      "[0.998732, 0.998732]\n",
      "counter ##################199######################\n",
      "[0.7047029, 0.7047029]\n",
      "counter ##################200######################\n",
      "[0.95465684, 0.95465684]\n",
      "counter ##################200######################\n",
      "[0.77338845, 0.77338845]\n",
      "counter ##################200######################\n",
      "[0.90872204, 0.90872204]\n",
      "counter ##################200######################\n",
      "[0.7473685, 0.7473685]\n",
      "counter ##################200######################\n",
      "[0.9987724, 0.9987724]\n",
      "counter ##################200######################\n",
      "[0.65679955, 0.65679955]\n",
      "counter ##################201######################\n",
      "[0.9543952, 0.9543952]\n",
      "counter ##################201######################\n",
      "[0.771236, 0.771236]\n",
      "counter ##################201######################\n",
      "[0.9078672, 0.9078672]\n",
      "counter ##################201######################\n",
      "[0.74134266, 0.74134266]\n",
      "counter ##################201######################\n",
      "[0.998808, 0.998808]\n",
      "counter ##################201######################\n",
      "[0.6797836, 0.6797836]\n",
      "counter ##################202######################\n",
      "[0.9541619, 0.9541619]\n",
      "counter ##################202######################\n",
      "[0.7633065, 0.7633065]\n",
      "counter ##################202######################\n",
      "[0.9077965, 0.9077965]\n",
      "counter ##################202######################\n",
      "[0.7438838, 0.7438838]\n",
      "counter ##################202######################\n",
      "[0.99884033, 0.99884033]\n",
      "counter ##################202######################\n",
      "[0.66569173, 0.66569173]\n",
      "counter ##################203######################\n",
      "[0.9539312, 0.9539312]\n",
      "counter ##################203######################\n",
      "[0.7579241, 0.7579241]\n",
      "counter ##################203######################\n",
      "[0.90808237, 0.90808237]\n",
      "counter ##################203######################\n",
      "[0.74627876, 0.74627876]\n",
      "counter ##################203######################\n",
      "[0.9988716, 0.9988716]\n",
      "counter ##################203######################\n",
      "[0.6576218, 0.6576218]\n",
      "counter ##################204######################\n",
      "[0.9536831, 0.9536831]\n",
      "counter ##################204######################\n",
      "[0.75674975, 0.75674975]\n",
      "counter ##################204######################\n",
      "[0.9077199, 0.9077199]\n",
      "counter ##################204######################\n",
      "[0.7538622, 0.7538622]\n",
      "counter ##################204######################\n",
      "[0.9989015, 0.9989015]\n",
      "counter ##################204######################\n",
      "[0.6503471, 0.6503471]\n",
      "counter ##################205######################\n",
      "[0.9534302, 0.9534302]\n",
      "counter ##################205######################\n",
      "[0.75611436, 0.75611436]\n",
      "counter ##################205######################\n",
      "[0.90803, 0.90803]\n",
      "counter ##################205######################\n",
      "[0.75580525, 0.75580525]\n",
      "counter ##################205######################\n",
      "[0.9989295, 0.9989295]\n",
      "counter ##################205######################\n",
      "[0.65214735, 0.65214735]\n",
      "counter ##################206######################\n",
      "[0.9532269, 0.9532269]\n",
      "counter ##################206######################\n",
      "[0.7329281, 0.7329281]\n",
      "counter ##################206######################\n",
      "[0.9088577, 0.9088577]\n",
      "counter ##################206######################\n",
      "[0.7552024, 0.7552024]\n",
      "counter ##################206######################\n",
      "[0.9989571, 0.9989571]\n",
      "counter ##################206######################\n",
      "[0.588577, 0.588577]\n",
      "counter ##################207######################\n",
      "[0.9529429, 0.9529429]\n",
      "counter ##################207######################\n",
      "[0.7284454, 0.7284454]\n",
      "counter ##################207######################\n",
      "[0.9079443, 0.9079443]\n",
      "counter ##################207######################\n",
      "[0.75201404, 0.75201404]\n",
      "counter ##################207######################\n",
      "[0.9989827, 0.9989827]\n",
      "counter ##################207######################\n",
      "[0.6576113, 0.6576113]\n",
      "counter ##################208######################\n",
      "[0.9527947, 0.9527947]\n",
      "counter ##################208######################\n",
      "[0.72690123, 0.72690123]\n",
      "counter ##################208######################\n",
      "[0.90805763, 0.90805763]\n",
      "counter ##################208######################\n",
      "[0.75491875, 0.75491875]\n",
      "counter ##################208######################\n",
      "[0.9990071, 0.9990071]\n",
      "counter ##################208######################\n",
      "[0.64002347, 0.64002347]\n",
      "counter ##################209######################\n",
      "[0.9526501, 0.9526501]\n",
      "counter ##################209######################\n",
      "[0.7241079, 0.7241079]\n",
      "counter ##################209######################\n",
      "[0.907257, 0.907257]\n",
      "counter ##################209######################\n",
      "[0.754986, 0.754986]\n",
      "counter ##################209######################\n",
      "[0.99902886, 0.99902886]\n",
      "counter ##################209######################\n",
      "[0.6361189, 0.6361189]\n",
      "counter ##################210######################\n",
      "[0.952529, 0.952529]\n",
      "counter ##################210######################\n",
      "[0.71573174, 0.71573174]\n",
      "counter ##################210######################\n",
      "[0.9074805, 0.9074805]\n",
      "counter ##################210######################\n",
      "[0.7436097, 0.7436097]\n",
      "counter ##################210######################\n",
      "[0.9990482, 0.9990482]\n",
      "counter ##################210######################\n",
      "[0.64419013, 0.64419013]\n",
      "counter ##################15######################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38569522, 0.38569522]\n",
      "counter ##################15######################\n",
      "[0.26761383, 0.26761383]\n",
      "counter ##################15######################\n",
      "[0.23353492, 0.23353492]\n",
      "counter ##################211######################\n",
      "[nan, nan]\n",
      "counter ##################211######################\n",
      "[nan, nan]\n",
      "counter ##################211######################\n",
      "[nan, nan]\n",
      "counter ##################211######################\n",
      "[nan, nan]\n",
      "counter ##################211######################\n",
      "[nan, nan]\n",
      "counter ##################211######################\n",
      "[nan, nan]\n",
      "counter ##################212######################\n",
      "[1.0000001, 1.0000001]\n",
      "counter ##################212######################\n",
      "[0.99999976, 0.99999976]\n",
      "counter ##################212######################\n",
      "[1.0, 1.0]\n",
      "counter ##################212######################\n",
      "[0.99997777, 0.99997777]\n",
      "counter ##################212######################\n",
      "[1.0, 1.0]\n",
      "counter ##################212######################\n",
      "[0.99877775, 0.99877775]\n",
      "counter ##################213######################\n",
      "[0.9771963, 0.9771963]\n",
      "counter ##################213######################\n",
      "[0.9927641, 0.9927641]\n",
      "counter ##################213######################\n",
      "[0.9588595, 0.9588595]\n",
      "counter ##################213######################\n",
      "[0.997289, 0.997289]\n",
      "counter ##################213######################\n",
      "[0.999316, 0.999316]\n",
      "counter ##################213######################\n",
      "[0.9772176, 0.9772176]\n",
      "counter ##################214######################\n",
      "[0.969927, 0.969927]\n",
      "counter ##################214######################\n",
      "[0.99103916, 0.99103916]\n",
      "counter ##################214######################\n",
      "[0.8897488, 0.8897488]\n",
      "counter ##################214######################\n",
      "[0.9748289, 0.9748289]\n",
      "counter ##################214######################\n",
      "[0.9979503, 0.9979503]\n",
      "counter ##################214######################\n",
      "[0.93329334, 0.93329334]\n",
      "counter ##################215######################\n",
      "[0.9706181, 0.9706181]\n",
      "counter ##################215######################\n",
      "[0.9839631, 0.9839631]\n",
      "counter ##################215######################\n",
      "[0.9068711, 0.9068711]\n",
      "counter ##################215######################\n",
      "[0.9754761, 0.9754761]\n",
      "counter ##################215######################\n",
      "[0.9966901, 0.9966901]\n",
      "counter ##################215######################\n",
      "[0.92460173, 0.92460173]\n",
      "counter ##################216######################\n",
      "[0.9711469, 0.9711469]\n",
      "counter ##################216######################\n",
      "[0.96290666, 0.96290666]\n",
      "counter ##################216######################\n",
      "[0.90930784, 0.90930784]\n",
      "counter ##################216######################\n",
      "[0.9731291, 0.9731291]\n",
      "counter ##################216######################\n",
      "[0.995319, 0.995319]\n",
      "counter ##################216######################\n",
      "[0.92363554, 0.92363554]\n",
      "counter ##################217######################\n",
      "[0.971196, 0.971196]\n",
      "counter ##################217######################\n",
      "[0.94026804, 0.94026804]\n",
      "counter ##################217######################\n",
      "[0.9058234, 0.9058234]\n",
      "counter ##################217######################\n",
      "[0.9096238, 0.9096238]\n",
      "counter ##################217######################\n",
      "[0.9948579, 0.9948579]\n",
      "counter ##################217######################\n",
      "[0.9264224, 0.9264224]\n",
      "counter ##################218######################\n",
      "[0.9700686, 0.9700686]\n",
      "counter ##################218######################\n",
      "[0.9326456, 0.9326456]\n",
      "counter ##################218######################\n",
      "[0.89979804, 0.89979804]\n",
      "counter ##################218######################\n",
      "[0.9031316, 0.9031316]\n",
      "counter ##################218######################\n",
      "[0.9949372, 0.9949372]\n",
      "counter ##################218######################\n",
      "[0.9220663, 0.9220663]\n",
      "counter ##################219######################\n",
      "[0.96887314, 0.96887314]\n",
      "counter ##################219######################\n",
      "[0.93500566, 0.93500566]\n",
      "counter ##################219######################\n",
      "[0.90849787, 0.90849787]\n",
      "counter ##################219######################\n",
      "[0.88909245, 0.88909245]\n",
      "counter ##################219######################\n",
      "[0.9950155, 0.9950155]\n",
      "counter ##################219######################\n",
      "[0.9297018, 0.9297018]\n",
      "counter ##################220######################\n",
      "[0.9685723, 0.9685723]\n",
      "counter ##################220######################\n",
      "[0.93490267, 0.93490267]\n",
      "counter ##################220######################\n",
      "[0.9135499, 0.9135499]\n",
      "counter ##################220######################\n",
      "[0.8891914, 0.8891914]\n",
      "counter ##################220######################\n",
      "[0.99518317, 0.99518317]\n",
      "counter ##################220######################\n",
      "[0.9303373, 0.9303373]\n",
      "counter ##################221######################\n",
      "[0.96736157, 0.96736157]\n",
      "counter ##################221######################\n",
      "[0.9294967, 0.9294967]\n",
      "counter ##################221######################\n",
      "[0.9023333, 0.9023333]\n",
      "counter ##################221######################\n",
      "[0.8806396, 0.8806396]\n",
      "counter ##################221######################\n",
      "[0.99540734, 0.99540734]\n",
      "counter ##################221######################\n",
      "[0.92603517, 0.92603517]\n",
      "counter ##################222######################\n",
      "[0.966988, 0.966988]\n",
      "counter ##################222######################\n",
      "[0.92885846, 0.92885846]\n",
      "counter ##################222######################\n",
      "[0.9013959, 0.9013959]\n",
      "counter ##################222######################\n",
      "[0.88268155, 0.88268155]\n",
      "counter ##################222######################\n",
      "[0.9957095, 0.9957095]\n",
      "counter ##################222######################\n",
      "[0.9126385, 0.9126385]\n",
      "counter ##################223######################\n",
      "[0.96649265, 0.96649265]\n",
      "counter ##################223######################\n",
      "[0.9278545, 0.9278545]\n",
      "counter ##################223######################\n",
      "[0.88948923, 0.88948923]\n",
      "counter ##################223######################\n",
      "[0.8851228, 0.8851228]\n",
      "counter ##################223######################\n",
      "[0.99603164, 0.99603164]\n",
      "counter ##################223######################\n",
      "[0.9354737, 0.9354737]\n",
      "counter ##################224######################\n",
      "[0.9655038, 0.9655038]\n",
      "counter ##################224######################\n",
      "[0.9251141, 0.9251141]\n",
      "counter ##################224######################\n",
      "[0.89366376, 0.89366376]\n",
      "counter ##################224######################\n",
      "[0.8765731, 0.8765731]\n",
      "counter ##################224######################\n",
      "[0.99638236, 0.99638236]\n",
      "counter ##################224######################\n",
      "[0.9141505, 0.9141505]\n",
      "counter ##################225######################\n",
      "[0.9651047, 0.9651047]\n",
      "counter ##################225######################\n",
      "[0.9203491, 0.9203491]\n",
      "counter ##################225######################\n",
      "[0.8964967, 0.8964967]\n",
      "counter ##################225######################\n",
      "[0.87466496, 0.87466496]\n",
      "counter ##################225######################\n",
      "[0.99666923, 0.99666923]\n",
      "counter ##################225######################\n",
      "[0.9247877, 0.9247877]\n",
      "counter ##################226######################\n",
      "[0.964892, 0.964892]\n",
      "counter ##################226######################\n",
      "[0.89866036, 0.89866036]\n",
      "counter ##################226######################\n",
      "[0.89907354, 0.89907354]\n",
      "counter ##################226######################\n",
      "[0.85708106, 0.85708106]\n",
      "counter ##################226######################\n",
      "[0.9969041, 0.9969041]\n",
      "counter ##################226######################\n",
      "[0.92572445, 0.92572445]\n",
      "counter ##################227######################\n",
      "[0.9644531, 0.9644531]\n",
      "counter ##################227######################\n",
      "[0.86161107, 0.86161107]\n",
      "counter ##################227######################\n",
      "[0.90137786, 0.90137786]\n",
      "counter ##################227######################\n",
      "[0.85550565, 0.85550565]\n",
      "counter ##################227######################\n",
      "[0.99711573, 0.99711573]\n",
      "counter ##################227######################\n",
      "[0.92801315, 0.92801315]\n",
      "counter ##################228######################\n",
      "[0.9637694, 0.9637694]\n",
      "counter ##################228######################\n",
      "[0.86265206, 0.86265206]\n",
      "counter ##################228######################\n",
      "[0.8982363, 0.8982363]\n",
      "counter ##################228######################\n",
      "[0.83243454, 0.83243454]\n",
      "counter ##################228######################\n",
      "[0.99731654, 0.99731654]\n",
      "counter ##################228######################\n",
      "[0.91704667, 0.91704667]\n",
      "counter ##################229######################\n",
      "[0.96314245, 0.96314245]\n",
      "counter ##################229######################\n",
      "[0.83049756, 0.83049756]\n",
      "counter ##################229######################\n",
      "[0.8926945, 0.8926945]\n",
      "counter ##################229######################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8271416, 0.8271416]\n",
      "counter ##################229######################\n",
      "[0.99747187, 0.99747187]\n",
      "counter ##################229######################\n",
      "[0.8962073, 0.8962073]\n",
      "counter ##################230######################\n",
      "[0.96282315, 0.96282315]\n",
      "counter ##################230######################\n",
      "[0.82824665, 0.82824665]\n",
      "counter ##################230######################\n",
      "[0.89714193, 0.89714193]\n",
      "counter ##################230######################\n",
      "[0.82611513, 0.82611513]\n",
      "counter ##################230######################\n",
      "[0.99759364, 0.99759364]\n",
      "counter ##################230######################\n",
      "[0.8880889, 0.8880889]\n",
      "counter ##################231######################\n",
      "[0.9625448, 0.9625448]\n",
      "counter ##################231######################\n",
      "[0.8259798, 0.8259798]\n",
      "counter ##################231######################\n",
      "[0.8890407, 0.8890407]\n",
      "counter ##################231######################\n",
      "[0.82738173, 0.82738173]\n",
      "counter ##################231######################\n",
      "[0.9976945, 0.9976945]\n",
      "counter ##################231######################\n",
      "[0.8833831, 0.8833831]\n",
      "counter ##################232######################\n",
      "[0.96211934, 0.96211934]\n",
      "counter ##################232######################\n",
      "[0.8235372, 0.8235372]\n",
      "counter ##################232######################\n",
      "[0.8879666, 0.8879666]\n",
      "counter ##################232######################\n",
      "[0.82668257, 0.82668257]\n",
      "counter ##################232######################\n",
      "[0.99782294, 0.99782294]\n",
      "counter ##################232######################\n",
      "[0.8845383, 0.8845383]\n",
      "counter ##################233######################\n",
      "[0.96168023, 0.96168023]\n",
      "counter ##################233######################\n",
      "[0.82254136, 0.82254136]\n",
      "counter ##################233######################\n",
      "[0.8890029, 0.8890029]\n",
      "counter ##################233######################\n",
      "[0.82283014, 0.82283014]\n",
      "counter ##################233######################\n",
      "[0.9979304, 0.9979304]\n",
      "counter ##################233######################\n",
      "[0.8808476, 0.8808476]\n",
      "counter ##################234######################\n",
      "[0.96115094, 0.96115094]\n",
      "counter ##################234######################\n",
      "[0.8177371, 0.8177371]\n",
      "counter ##################234######################\n",
      "[0.8847308, 0.8847308]\n",
      "counter ##################234######################\n",
      "[0.8235276, 0.8235276]\n",
      "counter ##################234######################\n",
      "[0.9980223, 0.9980223]\n",
      "counter ##################234######################\n",
      "[0.8809018, 0.8809018]\n",
      "counter ##################235######################\n",
      "[0.96079195, 0.96079195]\n",
      "counter ##################235######################\n",
      "[0.80782044, 0.80782044]\n",
      "counter ##################235######################\n",
      "[0.888452, 0.888452]\n",
      "counter ##################235######################\n",
      "[0.82493776, 0.82493776]\n",
      "counter ##################235######################\n",
      "[0.99811894, 0.99811894]\n",
      "counter ##################235######################\n",
      "[0.87809056, 0.87809056]\n",
      "counter ##################236######################\n",
      "[0.9602989, 0.9602989]\n",
      "counter ##################236######################\n",
      "[0.805778, 0.805778]\n",
      "counter ##################236######################\n",
      "[0.8888893, 0.8888893]\n",
      "counter ##################236######################\n",
      "[0.82959133, 0.82959133]\n",
      "counter ##################236######################\n",
      "[0.99819505, 0.99819505]\n",
      "counter ##################236######################\n",
      "[0.8653568, 0.8653568]\n",
      "counter ##################237######################\n",
      "[0.9598006, 0.9598006]\n",
      "counter ##################237######################\n",
      "[0.8062477, 0.8062477]\n",
      "counter ##################237######################\n",
      "[0.88972956, 0.88972956]\n",
      "counter ##################237######################\n",
      "[0.833169, 0.833169]\n",
      "counter ##################237######################\n",
      "[0.99828213, 0.99828213]\n",
      "counter ##################237######################\n",
      "[0.8790457, 0.8790457]\n",
      "counter ##################238######################\n",
      "[0.9592912, 0.9592912]\n",
      "counter ##################238######################\n",
      "[0.80349034, 0.80349034]\n",
      "counter ##################238######################\n",
      "[0.890161, 0.890161]\n",
      "counter ##################238######################\n",
      "[0.83018744, 0.83018744]\n",
      "counter ##################238######################\n",
      "[0.9983584, 0.9983584]\n",
      "counter ##################238######################\n",
      "[0.8829756, 0.8829756]\n",
      "counter ##################239######################\n",
      "[0.95880264, 0.95880264]\n",
      "counter ##################239######################\n",
      "[0.79633325, 0.79633325]\n",
      "counter ##################239######################\n",
      "[0.89323634, 0.89323634]\n",
      "counter ##################239######################\n",
      "[0.81514263, 0.81514263]\n",
      "counter ##################239######################\n",
      "[0.9984333, 0.9984333]\n",
      "counter ##################239######################\n",
      "[0.88566506, 0.88566506]\n",
      "counter ##################240######################\n",
      "[0.9583696, 0.9583696]\n",
      "counter ##################240######################\n",
      "[0.78940105, 0.78940105]\n",
      "counter ##################240######################\n",
      "[0.892666, 0.892666]\n",
      "counter ##################240######################\n",
      "[0.8146588, 0.8146588]\n",
      "counter ##################240######################\n",
      "[0.9984987, 0.9984987]\n",
      "counter ##################240######################\n",
      "[0.8803353, 0.8803353]\n",
      "counter ##################241######################\n",
      "[0.9579749, 0.9579749]\n",
      "counter ##################241######################\n",
      "[0.7867715, 0.7867715]\n",
      "counter ##################241######################\n",
      "[0.89300287, 0.89300287]\n",
      "counter ##################241######################\n",
      "[0.8096513, 0.8096513]\n",
      "counter ##################241######################\n",
      "[0.99855274, 0.99855274]\n",
      "counter ##################241######################\n",
      "[0.8791549, 0.8791549]\n",
      "counter ##################242######################\n",
      "[0.9576043, 0.9576043]\n",
      "counter ##################242######################\n",
      "[0.75607884, 0.75607884]\n",
      "counter ##################242######################\n",
      "[0.8923904, 0.8923904]\n",
      "counter ##################242######################\n",
      "[0.79480803, 0.79480803]\n",
      "counter ##################242######################\n",
      "[0.9986079, 0.9986079]\n",
      "counter ##################242######################\n",
      "[0.8798759, 0.8798759]\n",
      "counter ##################243######################\n",
      "[0.95717084, 0.95717084]\n",
      "counter ##################243######################\n",
      "[0.7507632, 0.7507632]\n",
      "counter ##################243######################\n",
      "[0.8945124, 0.8945124]\n",
      "counter ##################243######################\n",
      "[0.7740271, 0.7740271]\n",
      "counter ##################243######################\n",
      "[0.9986565, 0.9986565]\n",
      "counter ##################243######################\n",
      "[0.8781351, 0.8781351]\n",
      "counter ##################244######################\n",
      "[0.9567385, 0.9567385]\n",
      "counter ##################244######################\n",
      "[0.73650724, 0.73650724]\n",
      "counter ##################244######################\n",
      "[0.8961292, 0.8961292]\n",
      "counter ##################244######################\n",
      "[0.7730313, 0.7730313]\n",
      "counter ##################244######################\n",
      "[0.9987025, 0.9987025]\n",
      "counter ##################244######################\n",
      "[0.85781586, 0.85781586]\n",
      "counter ##################245######################\n",
      "[0.95629376, 0.95629376]\n",
      "counter ##################245######################\n",
      "[0.7300574, 0.7300574]\n",
      "counter ##################245######################\n",
      "[0.8983852, 0.8983852]\n",
      "counter ##################245######################\n",
      "[0.76578087, 0.76578087]\n",
      "counter ##################245######################\n",
      "[0.9987499, 0.9987499]\n",
      "counter ##################245######################\n",
      "[0.85904366, 0.85904366]\n",
      "counter ##################246######################\n",
      "[0.9559967, 0.9559967]\n",
      "counter ##################246######################\n",
      "[0.7254482, 0.7254482]\n",
      "counter ##################246######################\n",
      "[0.8984187, 0.8984187]\n",
      "counter ##################246######################\n",
      "[0.76768684, 0.76768684]\n",
      "counter ##################246######################\n",
      "[0.9987947, 0.9987947]\n",
      "counter ##################246######################\n",
      "[0.8642114, 0.8642114]\n",
      "counter ##################247######################\n",
      "[0.95573497, 0.95573497]\n",
      "counter ##################247######################\n",
      "[0.72278005, 0.72278005]\n",
      "counter ##################247######################\n",
      "[0.8992715, 0.8992715]\n",
      "counter ##################247######################\n",
      "[0.76886106, 0.76886106]\n",
      "counter ##################247######################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99883735, 0.99883735]\n",
      "counter ##################247######################\n",
      "[0.86588806, 0.86588806]\n",
      "counter ##################248######################\n",
      "[0.9553225, 0.9553225]\n",
      "counter ##################248######################\n",
      "[0.71362096, 0.71362096]\n",
      "counter ##################248######################\n",
      "[0.90112, 0.90112]\n",
      "counter ##################248######################\n",
      "[0.7672911, 0.7672911]\n",
      "counter ##################248######################\n",
      "[0.9988775, 0.9988775]\n",
      "counter ##################248######################\n",
      "[0.8654905, 0.8654905]\n",
      "counter ##################249######################\n",
      "[0.95496154, 0.95496154]\n",
      "counter ##################249######################\n",
      "[0.7088033, 0.7088033]\n",
      "counter ##################249######################\n",
      "[0.9014951, 0.9014951]\n",
      "counter ##################249######################\n",
      "[0.7648552, 0.7648552]\n",
      "counter ##################249######################\n",
      "[0.99891496, 0.99891496]\n",
      "counter ##################249######################\n",
      "[0.85938644, 0.85938644]\n",
      "counter ##################250######################\n",
      "[0.95465684, 0.95465684]\n",
      "counter ##################250######################\n",
      "[0.70684177, 0.70684177]\n",
      "counter ##################250######################\n",
      "[0.9033957, 0.9033957]\n",
      "counter ##################250######################\n",
      "[0.76113385, 0.76113385]\n",
      "counter ##################250######################\n",
      "[0.99894977, 0.99894977]\n",
      "counter ##################250######################\n",
      "[0.85702276, 0.85702276]\n",
      "counter ##################251######################\n",
      "[0.9543952, 0.9543952]\n",
      "counter ##################251######################\n",
      "[0.70682657, 0.70682657]\n",
      "counter ##################251######################\n",
      "[0.902777, 0.902777]\n",
      "counter ##################251######################\n",
      "[0.7614694, 0.7614694]\n",
      "counter ##################251######################\n",
      "[0.9989805, 0.9989805]\n",
      "counter ##################251######################\n",
      "[0.860247, 0.860247]\n",
      "counter ##################252######################\n",
      "[0.9541619, 0.9541619]\n",
      "counter ##################252######################\n",
      "[0.70987415, 0.70987415]\n",
      "counter ##################252######################\n",
      "[0.90282136, 0.90282136]\n",
      "counter ##################252######################\n",
      "[0.7395895, 0.7395895]\n",
      "counter ##################252######################\n",
      "[0.999009, 0.999009]\n",
      "counter ##################252######################\n",
      "[0.8434069, 0.8434069]\n",
      "counter ##################253######################\n",
      "[0.9539312, 0.9539312]\n",
      "counter ##################253######################\n",
      "[0.70355475, 0.70355475]\n",
      "counter ##################253######################\n",
      "[0.9033026, 0.9033026]\n",
      "counter ##################253######################\n",
      "[0.7357489, 0.7357489]\n",
      "counter ##################253######################\n",
      "[0.9990367, 0.9990367]\n",
      "counter ##################253######################\n",
      "[0.8574314, 0.8574314]\n",
      "counter ##################254######################\n",
      "[0.9536831, 0.9536831]\n",
      "counter ##################254######################\n",
      "[0.7024086, 0.7024086]\n",
      "counter ##################254######################\n",
      "[0.90310895, 0.90310895]\n",
      "counter ##################254######################\n",
      "[0.74129945, 0.74129945]\n",
      "counter ##################254######################\n",
      "[0.999062, 0.999062]\n",
      "counter ##################254######################\n",
      "[0.83523375, 0.83523375]\n",
      "counter ##################255######################\n",
      "[0.9534302, 0.9534302]\n",
      "counter ##################255######################\n",
      "[0.69903135, 0.69903135]\n",
      "counter ##################255######################\n",
      "[0.903403, 0.903403]\n",
      "counter ##################255######################\n",
      "[0.7408326, 0.7408326]\n",
      "counter ##################255######################\n",
      "[0.99908555, 0.99908555]\n",
      "counter ##################255######################\n",
      "[0.8494203, 0.8494203]\n",
      "counter ##################256######################\n",
      "[0.9532269, 0.9532269]\n",
      "counter ##################256######################\n",
      "[0.66396016, 0.66396016]\n",
      "counter ##################256######################\n",
      "[0.9044406, 0.9044406]\n",
      "counter ##################256######################\n",
      "[0.74003106, 0.74003106]\n",
      "counter ##################256######################\n",
      "[0.9991088, 0.9991088]\n",
      "counter ##################256######################\n",
      "[0.8319366, 0.8319366]\n",
      "counter ##################257######################\n",
      "[0.9529429, 0.9529429]\n",
      "counter ##################257######################\n",
      "[0.6598865, 0.6598865]\n",
      "counter ##################257######################\n",
      "[0.90368587, 0.90368587]\n",
      "counter ##################257######################\n",
      "[0.7335108, 0.7335108]\n",
      "counter ##################257######################\n",
      "[0.99913025, 0.99913025]\n",
      "counter ##################257######################\n",
      "[0.8332845, 0.8332845]\n",
      "counter ##################258######################\n",
      "[0.9527947, 0.9527947]\n",
      "counter ##################258######################\n",
      "[0.6591712, 0.6591712]\n",
      "counter ##################258######################\n",
      "[0.90386665, 0.90386665]\n",
      "counter ##################258######################\n",
      "[0.73899937, 0.73899937]\n",
      "counter ##################258######################\n",
      "[0.9991499, 0.9991499]\n",
      "counter ##################258######################\n",
      "[0.841623, 0.841623]\n",
      "counter ##################259######################\n",
      "[0.9526501, 0.9526501]\n",
      "counter ##################259######################\n",
      "[0.65384966, 0.65384966]\n",
      "counter ##################259######################\n",
      "[0.9032428, 0.9032428]\n",
      "counter ##################259######################\n",
      "[0.7282981, 0.7282981]\n",
      "counter ##################259######################\n",
      "[0.99916756, 0.99916756]\n",
      "counter ##################259######################\n",
      "[0.83916575, 0.83916575]\n",
      "counter ##################260######################\n",
      "[0.952529, 0.952529]\n",
      "counter ##################260######################\n",
      "[0.6527973, 0.6527973]\n",
      "counter ##################260######################\n",
      "[0.9035885, 0.9035885]\n",
      "counter ##################260######################\n",
      "[0.72864556, 0.72864556]\n",
      "counter ##################260######################\n",
      "[0.99918264, 0.99918264]\n",
      "counter ##################260######################\n",
      "[0.82967037, 0.82967037]\n",
      "BLEU score = 0.00\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpEfch_omGvW"
   },
   "source": [
    "Congratulations for finishing these tutorials! I hope you've found them useful.\n",
    "\n",
    "If you find any mistakes or want to ask any questions about any of the code or explanations used, feel free to submit a GitHub issue and I will try to correct it ASAP.\n",
    "\n",
    "## Appendix\n",
    "\n",
    "The `calculate_bleu` function above is unoptimized. Below is a significantly faster, vectorized version of it that should be used if needed. Credit for the implementation goes to [@azadyasar](https://github.com/azadyasar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "id": "HOqc7JSmmGvW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def translate_sentence_vectorized(src_tensor, src_field, trg_field, model, device, max_len=50):\n",
    "    assert isinstance(src_tensor, torch.Tensor)\n",
    "\n",
    "    model.eval()\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "    # enc_src = [batch_sz, src_len, hid_dim]\n",
    "\n",
    "    trg_indexes = [[trg_field.vocab.stoi[trg_field.init_token]] for _ in range(len(src_tensor))]\n",
    "    # Even though some examples might have been completed by producing a <eos> token\n",
    "    # we still need to feed them through the model because other are not yet finished\n",
    "    # and all examples act as a batch. Once every single sentence prediction encounters\n",
    "    # <eos> token, then we can stop predicting.\n",
    "    translations_done = [0] * len(src_tensor)\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).to(device)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        pred_tokens = output.argmax(2)[:,-1]\n",
    "        for i, pred_token_i in enumerate(pred_tokens):\n",
    "            trg_indexes[i].append(pred_token_i)\n",
    "            if pred_token_i == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "                translations_done[i] = 1\n",
    "        if all(translations_done):\n",
    "            break\n",
    "\n",
    "    # Iterate through each predicted example one by one;\n",
    "    # Cut-off the portion including the after the <eos> token\n",
    "    pred_sentences = []\n",
    "    for trg_sentence in trg_indexes:\n",
    "        pred_sentence = []\n",
    "        for i in range(1, len(trg_sentence)):\n",
    "            if trg_sentence[i] == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "                break\n",
    "            pred_sentence.append(trg_field.vocab.itos[trg_sentence[i]])\n",
    "        pred_sentences.append(pred_sentence)\n",
    "\n",
    "    return pred_sentences, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "id": "eZqfZ04LmGvW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu_alt(iterator, src_field, trg_field, model, device, max_len = 50):\n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            _trgs = []\n",
    "            for sentence in trg:\n",
    "                tmp = []\n",
    "                # Start from the first token which skips the <start> token\n",
    "                for i in sentence[1:]:\n",
    "                    # Targets are padded. So stop appending as soon as a padding or eos token is encountered\n",
    "                    if i == trg_field.vocab.stoi[trg_field.eos_token] or i == trg_field.vocab.stoi[trg_field.pad_token]:\n",
    "                        break\n",
    "                    tmp.append(trg_field.vocab.itos[i])\n",
    "                _trgs.append([tmp])\n",
    "            trgs += _trgs\n",
    "            pred_trg, _ = translate_sentence_vectorized(src, src_field, trg_field, model, device)\n",
    "            pred_trgs += pred_trg\n",
    "    return pred_trgs, trgs, bleu_score(pred_trgs, trgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "6 - Attention is All You Need.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
